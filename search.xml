<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>10.06</title>
    <url>/2025/10/06/10.06/</url>
    <content><![CDATA[<p><em>15:50</em><br>不要质疑自己好吗？你要知道你为什么数据结构的进度要落后一些，是因为你在权衡之后选择了要自学数据结构，选择了一条未知并且更难走的路，在这条路上，你已经付出了很多，你无数次想放弃，想改变学习路线，但你都坚持下来了，课外花了非常多的时间，所以你既然已经这么努力了，为什么还要总是内耗自己呢？</p>
<p>我知道你是一个敏感且感性的人，但是，请把情绪放在你一人身上，别人可能看起来进度比你快，但是！他绝对没有你努力！想想你开学以来所发出的这么多，机器学习深度学习，看完的网课读完的书，无数次内耗过后仍然选择站起来。</p>
<p>数据结构这一关对你来说不是一件小事吗？这两天花点时间把第三章网课看完然后准备课设，很难吗？不难啊！比它更难的事我们都已经做过无数次了，郑豪，从小到大，哪一个难关难倒你了？不论是复读还是上学期期末高数复习，组原复习以及组原课设熬的那几个夜晚，还有前几天的婚礼致辞，每一个看似很难的拦路虎，不都被你打倒了吗？</p>
<p>别人什么进度管我什么事啊，看到别人学习你只需要记住，不管他们怎么学，都绝对没有你努力，将来，也绝对不会有你成功，我郑豪，就是天下第一！</p>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>11.11</title>
    <url>/2025/11/11/11.11/</url>
    <content><![CDATA[<h3 id="GOOD"><a href="#GOOD" class="headerlink" title="GOOD"></a>GOOD</h3><ol>
<li>完成了论文总结初版(还剩精华版和markdown最终版嘿嘿)</li>
<li>离散数学和概率论讲真的我真的很累，但是我都尽力听了，我没有放弃，没有逃避，真的很棒！</li>
<li>虽然说最后半个小时没有花在科研上，但是我们知道了堆在优先级队列里，而且也再次明确了自己的学习路线，并没有浪费时间</li>
</ol>
<h3 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h3><ol>
<li>计网照听</li>
<li>习思想的时候把二叉搜索树听完</li>
<li>中午占座位</li>
<li>两点准时到图书馆写论文总结</li>
<li>高级搜索树能听多少是多少</li>
<li>晚自习把论文总结完成！</li>
</ol>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>1119科研</title>
    <url>/2025/11/19/1119%E7%A7%91%E7%A0%94/</url>
    <content><![CDATA[<blockquote>
<p>我的问题：模型A,B,C，它们的输入都是加了噪声的物性模型吗？</p>
</blockquote>
<p>这是一个非常好的问题，直击了级联模型中<strong>数据流和学习目标</strong>的核心！</p>
<p><strong>答案是：不是的。模型A、B、C的学习目标（也就是它们试图“去噪”并生成的东西）是完全不同的，因此它们在训练时所使用的“加了噪声的……”也是完全不同的。</strong></p>
<p>让我们来清晰地定义一下每个模型在训练时，到底在“折腾”什么数据。</p>
<hr>
<h3 id="模型A：宏观轮廓模型-学习“哪里有”"><a href="#模型A：宏观轮廓模型-学习“哪里有”" class="headerlink" title="模型A：宏观轮廓模型 (学习“哪里有”)"></a><strong>模型A：宏观轮廓模型 (学习“哪里有”)</strong></h3><ul>
<li><strong>它的“教材”是什么？</strong><ul>
<li>模型A的训练数据，是大量的<strong>低分辨率、二元（0或1）的“宏观相模型”</strong>。这些模型可以来自于对高质量合成数据的降采样，或者对地震属性进行粗略分割得到。</li>
</ul>
</li>
<li><strong>它加噪声的对象是什么？</strong><ul>
<li>它加噪声的对象，正是这些<strong>低分辨率的、只有0和1的“宏观相模型”</strong>。</li>
</ul>
</li>
<li><strong>训练过程：</strong><ol>
<li>拿一张<code>64x64x64</code>的、由0（非储层）和1（储层）组成的宏观相模型图。</li>
<li>向这张<strong>0/1的图</strong>上逐步添加高斯噪声。这个过程会将原本清晰的0和1，变成模糊的、-1到1之间的连续值。</li>
<li>训练模型A的神经网络，去学习如何从这个“模糊的连续值”图中，<strong>预测并还原</strong>出被添加的噪声，以便最终能恢复出清晰的0和1的结构。</li>
</ol>
</li>
<li><strong>总结：</strong> 模型A的输入，是<strong>加了噪声的“低分辨率、二元相模型”</strong>。它学习的是<strong>宏观尺度的空间布局规律</strong>。</li>
</ul>
<hr>
<h3 id="模型B：内部栅状结构模型-学习“内部是什么”"><a href="#模型B：内部栅状结构模型-学习“内部是什么”" class="headerlink" title="模型B：内部栅状结构模型 (学习“内部是什么”)"></a><strong>模型B：内部栅状结构模型 (学习“内部是什么”)</strong></h3><ul>
<li><strong>它的“教材”是什么？</strong><ul>
<li>模型B的训练数据，是大量的<strong>高分辨率、多元（0,1,2）的“精细相模型”</strong>。这些模型同样主要来自于高质量的合成数据。</li>
</ul>
</li>
<li><strong>它加噪声的对象是什么？</strong><ul>
<li>它加噪声的对象，是这些<strong>高分辨率的、由0（基岩）、1（裂缝带）、2（角砾带）组成的“精细相模型”</strong>。</li>
</ul>
</li>
<li><strong>训练过程：</strong><ol>
<li>拿一张<code>256x256x256</code>的、由0,1,2三种标签组成的精细相模型图。</li>
<li>向这张<strong>0/1/2的图</strong>上逐步添加高斯噪声。这同样会把清晰的整数标签，变成模糊的连续值。</li>
<li>训练模型B的神经网络，在<strong>模型A输出的宏观轮廓（作为条件）</strong>的指导下，去学习如何从这个模糊的图中，预测噪声，最终恢复出清晰的0,1,2的栅状结构。</li>
</ol>
</li>
<li><strong>总结：</strong> 模型B的输入，是<strong>加了噪声的“高分辨率、多元相模型”</strong>，以及一个<strong>作为条件的“低分辨率相模型”</strong>。它学习的是<strong>在宏观约束下，微观尺度的岩相组合模式</strong>。</li>
</ul>
<hr>
<h3 id="模型C：物性模型-学习“数值是多少”"><a href="#模型C：物性模型-学习“数值是多少”" class="headerlink" title="模型C：物性模型 (学习“数值是多少”)"></a><strong>模型C：物性模型 (学习“数值是多少”)</strong></h3><ul>
<li><strong>它的“教材”是什么？</strong><ul>
<li>模型C的训练数据，是大量的<strong>高分辨率、连续值的“物性模型”</strong>（比如孔隙度模型，里面的值是3.1%, 5.8%等）。</li>
</ul>
</li>
<li><strong>它加噪声的对象是什么？</strong><ul>
<li>它加噪声的对象，正是这些<strong>高分辨率的、值为连续浮点数的“物性模型”</strong>。</li>
</ul>
</li>
<li><strong>训练过程：</strong><ol>
<li>拿一张<code>256x256x256</code>的、值在0.0到0.1之间（代表0%到10%的孔隙度）的孔隙度模型图。</li>
<li>向这张<strong>连续值的图</strong>上逐步添加高斯噪声。</li>
<li>训练模型C的神经网络，在<strong>模型B输出的精细相模型（作为条件）</strong>的指导下，去学习如何从加噪后的孔隙度图中，预测噪声，最终恢复出符合相控规律的、数值分布合理的孔隙度场。</li>
</ol>
</li>
<li><strong>总结：</strong> 模型C的输入，才是我们一直讨论的<strong>加了噪声的“高分辨率、连续值物性模型”</strong>，以及一个<strong>作为条件的“高分辨率相模型”</strong>。它学习的是<strong>在岩相约束下，连续物理属性的空间分布规律</strong>。</li>
</ul>
<hr>
<p><strong>最终的清晰对应关系：</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">模型</th>
<th style="text-align:left">学习目标</th>
<th style="text-align:left">训练时“加噪声”的对象</th>
<th style="text-align:left">条件输入</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>模型A</strong></td>
<td style="text-align:left">宏观轮廓</td>
<td style="text-align:left">低分辨率、<strong>二元相模型</strong></td>
<td style="text-align:left">（可选）更宏观的地质约束</td>
</tr>
<tr>
<td style="text-align:left"><strong>模型B</strong></td>
<td style="text-align:left">内部栅状结构</td>
<td style="text-align:left">高分辨率、<strong>多元相模型</strong></td>
<td style="text-align:left">模型A的输出（低分辨率相模型）</td>
</tr>
<tr>
<td style="text-align:left"><strong>模型C</strong></td>
<td style="text-align:left">连续物性参数</td>
<td style="text-align:left">高分辨率、<strong>连续值物性模型</strong></td>
<td style="text-align:left">模型B的输出（高分辨率相模型）</td>
</tr>
</tbody>
</table>
</div>
<p>您的这个问题非常关键，它帮助我们精确地区分了级联流程中，每一个模型<strong>不同的“职责”和“学习材料”</strong>，让整个技术路线的逻辑变得更加严谨和清晰。</p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
  </entry>
  <entry>
    <title>10.12</title>
    <url>/2025/10/12/10.12/</url>
    <content><![CDATA[<p><em>15:33</em><br>郑豪，你想想你整个暑假做了多少事，学习各种工具，搭博客，学习机器学习，等等等等，你比别人领先了一个暑假，你慌什么啊，总是喜欢这样，就算你这个月不学习，那也比别人强，但是你有不学吗？你也在学啊，虽然最近时间没有规划好，但是没关系，你要相信，你就是最牛逼的，别总是内耗内耗内耗的，God is you，加油，你是最棒的，别人超过不了你的</p>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>8.31</title>
    <url>/2025/08/31/8.31/</url>
    <content><![CDATA[<h2 id="8-31"><a href="#8-31" class="headerlink" title="8.31"></a>8.31</h2><h3 id="三维地质建模，并且非常侧重于将人工智能（特别是深度学习）技术应用其中，实现地质建模的智能化。"><a href="#三维地质建模，并且非常侧重于将人工智能（特别是深度学习）技术应用其中，实现地质建模的智能化。" class="headerlink" title="三维地质建模，并且非常侧重于将人工智能（特别是深度学习）技术应用其中，实现地质建模的智能化。"></a>三维地质建模，并且非常侧重于将人工智能（特别是深度学习）技术应用其中，实现地质建模的智能化。</h3><ol>
<li><p>研究动机：为什么要进行三维地质建模？<br>这项研究有非常重要的实际应用价值，论文中提到了几个主要方向：<br>矿产勘查：通过模型找到地下哪里的矿产最丰富。<br>地质灾害预警：比如模拟和预测滑坡、地面沉降等灾害的发生。<br>城市地下空间规划：在修建地铁、地下管网前，先搞清楚地下的结构，避免风险。<br>油气资源开发：精确描绘油气储藏的形态和分布，提高开采效率。</p>
</li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>三天任务</title>
    <url>/2025/09/12/9.12~9.14%E4%B8%89%E5%A4%A9%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<p><em>前言：这几天节奏出了点问题，被一些事情影响了，但是没关系，从现在开始继续加油就完事儿了</em></p>
<h3 id="紧急："><a href="#紧急：" class="headerlink" title="紧急："></a>紧急：</h3><ol>
<li>数据结构学到单链表以及完成作业(周一之前要完成)</li>
<li>计算机网络第一章复习＋作业</li>
<li>英语读写教程(周末要提交)</li>
</ol>
<h3 id="稍缓："><a href="#稍缓：" class="headerlink" title="稍缓："></a>稍缓：</h3><ol>
<li>CS231n</li>
</ol>
<hr>
<h4 id="周五"><a href="#周五" class="headerlink" title="周五"></a>周五</h4><ol>
<li>周五上午三四节课完成计算机网络的复习</li>
<li>周五下午一二节课接着完成复习并完成作业</li>
<li>周五下午三四节课学习数据结构课本，第一章一定要结束，第二章要开头</li>
<li>周五晚上四个小时全学CS231n，如果累了就学会儿计网和数据结构</li>
<li>十点半去操场跑2km，或者操场上散步也行，23:10前回宿舍</li>
<li>回宿舍后把衣服送去洗衣机，然后洗澡</li>
<li>洗完澡后玩玩电脑，衣服洗好了拿去晒</li>
<li>十二点半之前睡觉</li>
</ol>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>一、Python学习中的两大法宝函数</title>
    <url>/2025/08/31/Python%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%B8%A4%E5%A4%A7%E6%B3%95%E5%AE%9D%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<ol>
<li>理解Package结构及法宝函数的作用<br><img src="/img/pyt_beg/p1.png" alt=""><br><img src="/img/pyt_beg/p2.png" alt=""></li>
<li>实战运用两大法宝函数<br>dir(torch)<br>dir(torch.cuda)<br>dir(torch.cuda.is_available)<br>help(torch.cuda.is_available)</li>
</ol>
]]></content>
      <categories>
        <category>深度学习入门</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>CS231n</title>
    <url>/2025/09/18/CS231n/</url>
    <content><![CDATA[<p>我们讨论过图像分类的概念，探讨了为何它如此困难，因为计算机看到的是庞大的数字网格，而你看到的是实际图像，这之间存在语义鸿沟</p>
<p>上次我们讨论了 k-最近邻分类器，作为对整个数据驱动思维方式的简单引入。</p>
]]></content>
      <categories>
        <category>深度学习入门</category>
      </categories>
  </entry>
  <entry>
    <title>markdown基本使用方法</title>
    <url>/2025/08/01/markdown%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h1 id="Markdown基本用法"><a href="#Markdown基本用法" class="headerlink" title="Markdown基本用法"></a>Markdown基本用法</h1><h2 id="1-标题"><a href="#1-标题" class="headerlink" title="1.标题"></a>1.标题</h2><p>井号的个数表示不同级的标题</p>
<h2 id="2-换行"><a href="#2-换行" class="headerlink" title="2.换行"></a>2.换行</h2><ol>
<li>换行注意要加两个空格，不然怕在其他渲染器里失效</li>
<li>如果中间空一行，那就是新起一段</li>
</ol>
<h2 id="3-强调（加粗和斜体）"><a href="#3-强调（加粗和斜体）" class="headerlink" title="3.强调（加粗和斜体）"></a>3.强调（加粗和斜体）</h2><ol>
<li>加粗：左右各加两个*</li>
<li>斜体：左右各加一个*</li>
<li>快捷键（前提安装了扩展）<ol>
<li>斜体：ctrl+i</li>
<li>加粗：ctrl+b</li>
</ol>
</li>
</ol>
<h2 id="4-列表"><a href="#4-列表" class="headerlink" title="4.列表"></a>4.列表</h2><ol>
<li>tab键可以继续缩进列表</li>
<li>列表编号你自己写的数字不用在意，系统会自动渲染成正确的顺序</li>
<li>如果要重新从1开始哦编号，只需要在两个列表之间加一个段落就行</li>
</ol>
<h2 id="5-图片"><a href="#5-图片" class="headerlink" title="5.图片"></a>5.图片</h2><p>先把图片保存到文件夹，然后输入![]（） 括号里在加上一个英文句号，然后就可以选择照片</p>
<h2 id="6-插入公式"><a href="#6-插入公式" class="headerlink" title="6.插入公式"></a>6.插入公式</h2><ol>
<li>用$$括起来即可</li>
<li>markdown all in one可以提供自动补全(怎么失效了┭┮﹏┭┮)</li>
<li>文字中插入公式：ctrl+m<br>注意在第一个美元符号前加空格，以保证兼容性</li>
<li>如果按两下ctrl+m，那就是单独一段的公式</li>
</ol>
<h2 id="7-表格"><a href="#7-表格" class="headerlink" title="7.表格"></a>7.表格</h2><ol>
<li>表格第一行代表标头</li>
<li>用  |   隔开</li>
<li>表头下面需要加上一行—-|—-|—-</li>
<li>表格默认左对齐，—-:右对齐，:—-:居中对齐</li>
<li>alt+shift+f 格式化，使表格在文本中变得好看一点</li>
</ol>
<h2 id="8-链接"><a href="#8-链接" class="headerlink" title="8.链接"></a>8.链接</h2><ol>
<li>把链接复制之后ctrl+v到选中的文字（安装扩展后）</li>
</ol>
<h2 id="9-代码块"><a href="#9-代码块" class="headerlink" title="9.代码块"></a>9.代码块</h2><ol>
<li>一对```</li>
<li>后面加上使用的编程语言名称即可实现高亮</li>
</ol>
<h2 id="10-导出为pdf"><a href="#10-导出为pdf" class="headerlink" title="10.导出为pdf"></a>10.导出为pdf</h2><p>按此方法能保证公式渲染正确</p>
<h2 id="11-其他语法"><a href="#11-其他语法" class="headerlink" title="11.其他语法"></a>11.其他语法</h2><p>这里是<a href="https://markdown.com.cn/basic-syntax/">链接</a><br>如果latex公式出现问题，请访问此<a href="https://rickliu.com/posts/f9538327001b/index.html">链接</a></p>
<blockquote>
<p>最后来张图片~<br><img src="/img/Yojiro.jpg" alt=""></p>
</blockquote>
]]></content>
      <categories>
        <category>工具使用教程</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>二、pycharm与jupyter使用与对比</title>
    <url>/2025/08/31/pycharm%E4%B8%8Ejupyter%E4%BD%BF%E7%94%A8%E4%B8%8E%E5%AF%B9%E6%AF%94/</url>
    <content><![CDATA[<p> 我不用pycharm，它的两种方法与vscode一样的<br> jupyter notebook的话，先打开Anaconda Prompt,然后输入如图所示代码<br> <img src="/img/pyt_beg/p3.png" alt=""></p>
<p> 三者优缺点：<br>  <img src="/img/pyt_beg/p4.png" alt=""></p>
]]></content>
      <categories>
        <category>深度学习入门</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>三、pytorch加载数据初认识</title>
    <url>/2025/08/31/pytorch%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E5%88%9D%E8%AE%A4%E8%AF%86/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>深度学习入门</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>vscode使用pytorch</title>
    <url>/2025/08/31/vscode%E4%BD%BF%E7%94%A8pytorch/</url>
    <content><![CDATA[<p>好的，这是一个非常经典且实用的组合。在VS Code中使用已经配置好的Anaconda和PyTorch环境，主要分为以下几个关键步骤。我会为你提供详细的指引。</p>
<h3 id="核心步骤概览"><a href="#核心步骤概览" class="headerlink" title="核心步骤概览"></a>核心步骤概览</h3><ol>
<li><strong>安装VS Code的Python扩展：</strong> 这是让VS Code识别和管理Python环境的基础。</li>
<li><strong>在VS Code中选择正确的Conda环境：</strong> 确保你的VS Code使用的是安装了PyTorch的那个Anaconda环境。</li>
<li><strong>验证环境是否配置成功：</strong> 在VS Code中运行代码，检查PyTorch能否被正确导入和使用。</li>
</ol>
<hr>
<h3 id="详细操作指南"><a href="#详细操作指南" class="headerlink" title="详细操作指南"></a>详细操作指南</h3><h4 id="步骤一：安装VS-Code的Python扩展"><a href="#步骤一：安装VS-Code的Python扩展" class="headerlink" title="步骤一：安装VS Code的Python扩展"></a>步骤一：安装VS Code的Python扩展</h4><p>如果你还没有安装，这是第一步，也是最重要的一步。</p>
<ol>
<li>打开VS Code。</li>
<li>点击左侧边栏的<strong>扩展(Extensions)</strong>图标（看起来像四个方块）。</li>
<li>在搜索框中输入 <code>Python</code>。</li>
<li>找到由<strong>Microsoft</strong>发布的Python扩展，点击<strong>安装(Install)</strong>。</li>
</ol>
<h4 id="步骤二：在VS-Code中选择你的Conda环境"><a href="#步骤二：在VS-Code中选择你的Conda环境" class="headerlink" title="步骤二：在VS Code中选择你的Conda环境"></a>步骤二：在VS Code中选择你的Conda环境</h4><p>安装好Python扩展后，VS Code就能够识别你的Anaconda环境了。现在你需要告诉它具体使用哪一个。</p>
<ol>
<li><p><strong>打开或创建一个Python项目：</strong></p>
<ul>
<li>在VS Code中，通过 <code>文件(File)</code> &gt; <code>打开文件夹(Open Folder)</code> 打开一个你的项目文件夹。</li>
<li>在文件夹中创建一个Python文件，例如 <code>test.py</code>。</li>
</ul>
</li>
<li><p><strong>选择Python解释器：</strong></p>
<ul>
<li>按下快捷键 <code>Ctrl+Shift+P</code> (在macOS上是 <code>Cmd+Shift+P</code>) 打开命令面板。</li>
<li>在命令面板中输入并选择 <strong><code>Python: Select Interpreter</code></strong> (Python: 选择解释器)。</li>
<li>稍等片刻，VS Code会列出它找到的所有Python环境，包括你的Anaconda环境。</li>
<li>从列表中找到并选择你<strong>安装了PyTorch的那个Conda环境</strong>。它通常会带有 <code>(&#39;your_env_name&#39;: conda)</code> 这样的标识。如果你没有为PyTorch创建新环境，就选择 <code>base</code> 环境。</li>
</ul>
</li>
<li><p><strong>激活终端中的Conda环境：</strong></p>
<ul>
<li>选择了解释器后，打开一个新的VS Code集成终端 (<code>视图(View)</code> &gt; <code>终端(Terminal)</code> 或快捷键 <code>Ctrl+`` </code>)。</li>
<li>通常情况下，VS Code会自动为你激活所选的Conda环境。你会看到终端提示符前面有 <code>(your_env_name)</code> 的字样，例如 <code>(base) C:\Users\YourName\Project&gt;</code>。</li>
<li>如果终端没有自动激活，你可以在终端中手动运行 <code>conda activate your_env_name</code> 来激活（将 <code>your_env_name</code> 替换为你的环境名）。</li>
</ul>
</li>
</ol>
<h4 id="步骤三：验证PyTorch是否可用"><a href="#步骤三：验证PyTorch是否可用" class="headerlink" title="步骤三：验证PyTorch是否可用"></a>步骤三：验证PyTorch是否可用</h4><p>最后一步是写一小段代码来确认一切都已就绪。</p>
<ol>
<li><p>在你创建的 <code>test.py</code> 文件中，输入以下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;PyTorch Version: <span class="hljs-subst">&#123;torch.__version__&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;CUDA is available: <span class="hljs-subst">&#123;torch.cuda.is_available()&#125;</span>&quot;</span>) <span class="hljs-comment"># 检查GPU是否可用</span><br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;CUDA Version: <span class="hljs-subst">&#123;torch.version.cuda&#125;</span>&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Current GPU: <span class="hljs-subst">&#123;torch.cuda.get_device_name(torch.cuda.current_device())&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 创建一个张量(tensor)</span><br>x = torch.rand(<span class="hljs-number">5</span>, <span class="hljs-number">3</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nA random tensor:&quot;</span>)<br><span class="hljs-built_in">print</span>(x)<br></code></pre></td></tr></table></figure>
</li>
<li><p><strong>运行代码：</strong></p>
<ul>
<li>在文件编辑区点击鼠标右键，选择 <strong><code>在终端中运行 Python 文件 (Run Python File in Terminal)</code></strong>。</li>
<li>或者点击VS Code右上角的绿色三角形“运行”按钮。</li>
</ul>
</li>
<li><p><strong>检查输出：</strong></p>
<ul>
<li>观察VS Code下方的终端窗口。如果一切正常，它会打印出你安装的PyTorch版本号、CUDA是否可用以及一个随机生成的张量，不会报告 <code>ModuleNotFoundError</code>。</li>
</ul>
</li>
</ol>
<h3 id="常见问题与解决方法"><a href="#常见问题与解决方法" class="headerlink" title="常见问题与解决方法"></a>常见问题与解决方法</h3><ul>
<li><p><strong>问题：VS Code找不到我的Conda环境。</strong></p>
<ul>
<li><strong>解决方法：</strong> 确保Anaconda已正确添加到系统的环境变量(PATH)中。 或者，在VS Code的设置(Settings)中搜索 <code>python.condaPath</code>，并手动指定你的 <code>conda.exe</code> 文件的路径。</li>
</ul>
</li>
<li><p><strong>问题：选择了环境，但在终端中 <code>import torch</code> 仍然失败。</strong></p>
<ul>
<li><strong>解决方法：</strong> 确认VS Code右下角状态栏显示的Python解释器版本，是否确实是你安装了PyTorch的那个环境。 同时，检查终端提示符前是否显示了正确的 <code>(your_env_name)</code>。如果不是，请手动激活。</li>
</ul>
</li>
<li><p><strong>问题：Jupyter Notebook (.ipynb) 文件无法使用Conda核。</strong></p>
<ul>
<li><strong>解决方法：</strong><ol>
<li>确保你也安装了VS Code的 <strong>Jupyter</strong> 扩展。</li>
<li>在<code>.ipynb</code>文件中，点击右上角的 <strong><code>选择内核(Select Kernel)</code></strong>。</li>
<li>在弹出的列表中选择 <strong><code>Python Environments...</code></strong>，然后找到并选择你的Conda环境。</li>
</ol>
</li>
</ul>
</li>
</ul>
<p>通过以上步骤，你就可以顺利地在VS Code这个强大的编辑器中，利用Anaconda管理的环境和PyTorch库进行深度学习开发了。祝你编码愉快！</p>
]]></content>
      <categories>
        <category>深度学习入门</category>
      </categories>
      <tags>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title>分类问题</title>
    <url>/2025/08/10/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><h3 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h3><p>要预测的变量y是一个离散值</p>
<ol>
<li><p>正类/负类</p>
<p>一般来说，负类表示没有某样东西</p>
</li>
<li><p>不推荐将线性回归用于分类问题</p>
</li>
</ol>
<hr>
<p>对于标签y为离散值0或1，我们使用分类算法——<strong>logistic回归算法</strong>  </p>
<p>特点是算法的输出或者说预测值一直介于0和1之间，不会大于1或者小于0</p>
<p>还有，虽然名字中带有“回归”二字，但是它是一种分类算法，叫做这个只是因为历史原因<br><img src="/img/ML/p4.png" alt=""></p>
<hr>
<h3 id="logistic回归"><a href="#logistic回归" class="headerlink" title="logistic回归"></a>logistic回归</h3><h4 id="part-1-假设函数的表示方法"><a href="#part-1-假设函数的表示方法" class="headerlink" title="part 1 假设函数的表示方法"></a>part 1 假设函数的表示方法</h4><p><img src="/img/ML/p5.png" alt=""><br><img src="/img/ML/p6.png" alt=""><br>假设函数解读：给定参数 $\theta$,对具有 $x$特征的病人， $y=1$的概率</p>
<h4 id="part-2-决策边界的概念"><a href="#part-2-决策边界的概念" class="headerlink" title="part 2 决策边界的概念"></a>part 2 决策边界的概念</h4><p><em>假设函数是如何做出预测的</em><br><img src="/img/ML/p7.png" alt=""></p>
<p><em>决策边界例子</em><br><img src="/img/ML/p8.png" alt=""><br><img src="/img/ML/p9.png" alt=""><br>注意，决策边界是假设函数的一个属性，由参数 $\theta$决定，<strong>它不是数据集的属性！</strong></p>
<h4 id="part-3-如何拟合logistic回归模型的参数-theta"><a href="#part-3-如何拟合logistic回归模型的参数-theta" class="headerlink" title="part 3 如何拟合logistic回归模型的参数 $\theta$"></a>part 3 如何拟合logistic回归模型的参数 $\theta$</h4><ol>
<li><p>代价函数<br><img src="/img/ML/p10.png" alt=""><br><em>保证是凸函数</em></p>
</li>
<li><p>找出使J($\theta$)最小的 $\theta$</p>
<p>我们要知道，假设的输出实际上就是在输入为$x$，并且以 $\theta$为参数时使 $y=1$的概率，</p>
<p>最小化代价函数的方法是——<strong>梯度下降法</strong><br><img src="/img/ML/p11.png" alt=""></p>
<p>如果有n个特征，那就有一个 $(n+1)$维参数向量 $\theta$。通过上面那个式子来同时更新所有 $\theta$的值（从0到n）</p>
<p>注意别把此处的更新规则与线性回归的混为一谈，二者看似形式一样，但是此处的假设函数 $h(\theta)$的定义已经发生了变化<br><img src="/img/ML/p12.png" alt=""></p>
<p>拓：不要用for循环实现参数更新，效率太低，用向量化来实现把n+1个参数同时更新</p>
</li>
</ol>
<h4 id="part-4-多类别分类问题"><a href="#part-4-多类别分类问题" class="headerlink" title="part 4 多类别分类问题"></a>part 4 多类别分类问题</h4><p><strong>通过“一对多”的分类算法实现</strong></p>
<p>比如说这个例子，我们要将训练集转化为三个独立的二元分类问题<br><img src="/img/ML/p13.png" alt=""></p>
<p>在三个分类器中运行输入x，然后选择 $h$最大的类别作为预测值</p>
<h4 id="part-5-过拟合问题"><a href="#part-5-过拟合问题" class="headerlink" title="part 5 过拟合问题"></a>part 5 过拟合问题</h4><p><img src="/img/ML/p14.png" alt=""><br><img src="/img/ML/p15.png" alt=""><br><img src="/img/ML/p16.png" alt=""></p>
<p>当我们使用一维或二维数据时，我们可以通过绘出假设模型的图像来研究问题所在，再选择合适的多项式阶数，<strong>但是这并不总是有用</strong></p>
<p>更多的时候，我们的学习问题需要有很多特征变量，并且这不仅仅是选择多项式阶次的问题，。当特征变量很多时，绘图变得更难，通过数据的可视化来决定保留哪些特征变量也更难</p>
<p>但是，如果我们有过多的变量而只有非常少的训练数据，就会出现<strong>过拟合</strong>的问题</p>
<p>有两个办法来解决过拟合的问题</p>
<p>第一个办法是尽量减少选取的变量，但不推荐！！！因为担心丢失了有用的信息</p>
<p>第二个才是我们主要的方法，<strong>正则化</strong></p>
<h4 id="part-6-正则化"><a href="#part-6-正则化" class="headerlink" title="part 6 正则化"></a>part 6 正则化</h4><p><strong>核心：我们将保留所有的特征变量，但是减少量级(或者说是参数 $\theta_j$的大小)</strong></p>
<p>当我们进行正则化的时候，我们还将写出相应的代价函数，关键是加入<strong>惩罚项</strong><br><img src="/img/ML/p17.png" alt=""><br>核心思想就是使参数尽量地小，以使假设模型更简单</p>
<p>由于在实际情况中，我们不知道到底哪些参数要缩小，所以我们要通过一个代价函数来对所有参数进行一个操作(在原来的代价函数后面添加一个新的项——<strong>正则化项</strong>，注意求和是从1开始，不从0开始)</p>
<p>$\lambda$被称为正则化参数，作用是控制两个不同目标之间的取舍——拟合数据与保持参数尽量地小(保持假设模型的相对简单，避免过拟合的情况)<br><img src="/img/ML/p18.png" alt=""><br><em>优化后的曲线并不是二次函数，但是却相对更平滑，更简单</em></p>
<p>如果$\lambda$被设得太大的话，那么对每个参数的惩罚程度就太大了，都趋近于0，最后只剩$\theta_0$，变成用一条直线去拟合数据了，这就是一个<strong>欠拟合</strong>的例子，因此，正则化参数$\lambda$的选择尤其重要，接下来我们就会讲到如何自动地选择它</p>
<h4 id="part-7-线性回归的正则化"><a href="#part-7-线性回归的正则化" class="headerlink" title="part 7 线性回归的正则化"></a>part 7 线性回归的正则化</h4><h5 id="算法一：梯度下降法"><a href="#算法一：梯度下降法" class="headerlink" title="算法一：梯度下降法"></a>算法一：梯度下降法</h5><p>把$\theta_0$的更新单独写出来，因为它不需要被惩罚</p>
<p>当我们进行正则化线性回归时，我们要做的就是每次迭代时，都将$\theta_j$乘以一个比1略小的数。从数学的角度来看，我们做的就是对代价函数进行梯度下降<br><img src="/img/ML/p19.png" alt=""></p>
<h5 id="算法二：正规方程"><a href="#算法二：正规方程" class="headerlink" title="算法二：正规方程"></a>算法二：正规方程</h5><p>我们的做法就是建立一个设计矩阵$X$，它的每一行都代表一个单独的训练样本。然后建立一个向量$y$，它是一个m维的向量，包含了训练集里的所有标签。</p>
<p>所以$X$是一个$m\times(n+1)$维的矩阵，y是一个m维的向量<br><img src="/img/ML/p20.png" alt=""></p>
<p>最后再来谈谈不可逆的问题，如果m小于n，那么$(X^TX)$是不可逆的(奇异矩阵，矩阵退化)</p>
<p>幸运的是，在正则化中已经考虑到了这个问题，<strong>只要正则化参数$\lambda$是严格大于0的，我们就可以确定$(X^TX)$+后面那个矩阵，得到的一定不是奇异矩阵，一定是可逆的！</strong><br><img src="/img/ML/p21.png" alt=""><br>因此，进行正则化还可以解决一些X的转置乘X出现不可逆的问题</p>
<p>好了，我们学会了实现正则化线性回归，利用它，我们就能避免出现过拟合的问题，即使在一个很小的训练集里拥有大量的特征</p>
<h4 id="part-8-逻辑回归的正则化"><a href="#part-8-逻辑回归的正则化" class="headerlink" title="part 8 逻辑回归的正则化"></a>part 8 逻辑回归的正则化</h4><p><img src="/img/ML/p22.png" alt=""><br>总体思路与线性回归的正则化中的梯度下降法相同，但是要注意虽然二者形式看似相同，但$h_\theta(x)$并不一样！</p>
<p>还有哦，刚才在线性回归里忘记强调了，方括号里的式子就是代价函数对$\theta_j$的偏导数</p>
<p>拓：<br><img src="/img/ML/p37.png" alt=""></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>分类问题</tag>
      </tags>
  </entry>
  <entry>
    <title>向量化</title>
    <url>/2025/08/10/%E5%90%91%E9%87%8F%E5%8C%96/</url>
    <content><![CDATA[<h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><ol>
<li>通过编程环境内置的或者说易于获取的线性代数库，而不是自己去编写，可以大大加快运行速度(尤其是在特征量非常大的时候)，并且更加有效利用计算机的并行硬件系统</li>
<li>代码更少，出错概率小!<br><img src="/img/ML/p2.png" alt=""><br><img src="/img/ML/p3.png" alt=""></li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>向量化</tag>
      </tags>
  </entry>
  <entry>
    <title>大二上规划</title>
    <url>/2025/10/20/%E5%A4%A7%E4%BA%8C%E4%B8%8A%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<blockquote>
<p>最近总是很焦虑，尤其是看到以前不如自己的一些人逐渐地发光发热，归根结底还是因为自己的路已经和他们不同了，这没办法，过去的事情无法改变，但是我们可以把握当下，展望未来，嗯，既然这样，那就好好修改一下目前的学习计划吧，这两个月以来的方向出了点小偏差，也不怪你，换谁谁都迷茫，你选择了坚持地走下去已经非常非常非常不错了</p>
</blockquote>
<h3 id="大英"><a href="#大英" class="headerlink" title="大英"></a>大英</h3><p>这学期对大英的要求稍微低一点，四级的话能过就行，重要的是之后的六级。<br>由于英语老师讲课节奏慢，又把大部分精力放在六级词汇上，但是这学期要考的是四级，而且我课下无法花时间再去记了，所以计划如下：</p>
<ol>
<li>周一英语课，坐后排偏左侧位置，如果老师讲的是废物内容，那就不听，直接记单词，如果老师一直讲六级词汇，也不听，直接记上面的四级单词，但是课文要读，不认识的词汇但感觉常见的要查</li>
<li>周四口语课，暂置，还没上，不知道怎么样</li>
<li>考前一两周左右再开始每天听点听力，做会儿阅读，研究会儿作文，不慌，能过就行</li>
</ol>
<h3 id="计算机网络"><a href="#计算机网络" class="headerlink" title="计算机网络"></a>计算机网络</h3><p>服了，我现在也很纠结，到底以梁老师为主还是郑老师为主，郑老师的太长了，算了，先试一下这种方法</p>
<ol>
<li>课前预习就湖工大，然后听梁老师讲，没听懂的做好标记，课下研究，等发课件</li>
<li>周三的课做计网作业</li>
</ol>
<h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><ol>
<li>接着听邓老师的课昂</li>
<li>周二音乐剧赏析把电脑带去，做作业</li>
</ol>
<h3 id="离散数学"><a href="#离散数学" class="headerlink" title="离散数学"></a>离散数学</h3><p>课前预习好，我觉得至少得领先老师一小节，然后的话，没搞懂的地方，下去一定要搞懂，别拖啊，当天晚上就搞明白，绝对不要拖，拖到后面根本不想动</p>
<h3 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h3><p>课前也得预习，ppt看快点，然后就是作业，一定要对答案，做错了的，没搞懂的一定得搞懂，并没有叫你花大量时间，但是得把现有的做好啊</p>
<h2 id="水课以及课余时间安排"><a href="#水课以及课余时间安排" class="headerlink" title="水课以及课余时间安排"></a>水课以及课余时间安排</h2><ol>
<li><p>周一晚：<br>预习离散数学，预习概率论，完成概率论作业</p>
</li>
<li><p>周二上午：<br>搞科研</p>
</li>
<li><p>周二晚：<br>晚自习就把电脑带去，做数据结构作业<br>来到图书馆后，先预习一下计网，然后就搞科研</p>
</li>
<li><p>周三思政：<br>听数据结构网课</p>
</li>
<li><p>周三下午5,6节课：<br>搞科研</p>
</li>
<li><p>周三晚：<br>计网作业，九点来图书馆后搞科研</p>
</li>
<li><p>周四上午1,2节课与下午与晚上：<br>搞科研，累了就听数据结构网课，做数据结构作业</p>
</li>
<li><p>周五上午1,2节课：<br>搞科研</p>
</li>
<li><p>周五上午思政：<br> 离散数学预习+概率论作业</p>
</li>
<li><p>周六周日：<br>主要是搞科研，学累了就看会儿数据结构网课，做会儿数据结构、计网作业，注意周日晚上要预习计网</p>
</li>
<li><p>每天早上必须七点半起床！硬性要求！我绝对不允许你再迟于七点半起床了，不然真的是连锁反应，吃不了早饭记不了单词，去图书馆又去的很晚，难受啊哥们儿</p>
</li>
<li><p>每天复习单词＋记新的20个单词，硬性要求，绝对要完成啊 </p>
</li>
</ol>
<h2 id="休闲"><a href="#休闲" class="headerlink" title="休闲"></a>休闲</h2><p>讲道理，这方面我真不太想硬性要求，但是如果不好好安排的话，有时候休息反而觉得煎熬，因为感觉花了时间但却感觉啥也没做</p>
<p>一天之中有哪几段休息的时间呢？<br>午饭、晚饭、晚上回寝室</p>
<p>休息手段有哪几种呢？<br>刷短视频，看电影解说，看动漫，看电影，玩游戏</p>
<p>游戏分为哪几种呢？<br>竞技类游戏，剧情类游戏</p>
<p>午饭：看动漫吧(因为电脑经常在图书馆)，好多动漫看一点就没看了，未闻花名，JOJO，野良神，尤其JOJO噢，看嘿</p>
<p>晚饭：与午饭同理</p>
<p>晚归：可以玩石头门，可以看动漫，但不要玩竞技类游戏，完全是浪费时间，它和无目的地刷视频是最没有意义的东西</p>
<h2 id="其他的一些补充"><a href="#其他的一些补充" class="headerlink" title="其他的一些补充"></a>其他的一些补充</h2><ol>
<li>平常晚上十一点半洗漱，十二点上床，十二点半前一定要睡觉</li>
<li>洗澡日，十一点洗漱，十二点上床，十二点半前睡觉</li>
<li>枕套两天一换，床单两周一换</li>
<li>暂时，阿达帕林-休息-水杨酸-休息-阿达帕林….</li>
<li>如果说没啥紧急任务，学得也差不多，周六周日可以出去吃吃饭，逛一逛，感受一下生活嘛，世界是很精彩的</li>
</ol>
<blockquote>
<p>最后想送给你几句话</p>
<ol>
<li>如果你内耗，请写下来，理性分析，这样你就知道到底有没有内耗的必要了</li>
<li>请相信，你就是最努力的，你就是最棒的，想想你暑假所付出的，想想你平常所付出的，想想你漏考后仍然站起来往前走</li>
<li>没有什么困难能难倒你，复读那一年所经历的磨难你都挺过来了，这世界上还能有比它更难的事了吗？没有！<br>大一上学期的痛苦期末月，起早贪黑地学c++，大一下学期一天复习完高数，以及那几天通宵做计组课设，几度想放弃但始终坚持下去的你，我去，真的，我说真的，谁能比的上你啊，你的精神，这种不屈的精神，无人能及！</li>
<li>觉得社恐的时候想想你在哥哥婚礼上的演讲，台下所有人的赞叹，我去，还有什么能让我害怕的啊！</li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>日记</category>
      </categories>
  </entry>
  <entry>
    <title>本科生科研CV</title>
    <url>/2025/08/30/%E6%9C%AC%E7%A7%91%E7%94%9F%E7%A7%91%E7%A0%94CV/</url>
    <content><![CDATA[<h2 id="本科生科研CV"><a href="#本科生科研CV" class="headerlink" title="本科生科研CV"></a>本科生科研CV</h2><ol>
<li>不要超过一页，word文档一面就行</li>
<li>东西要写的简单</li>
<li>从你做的最好的然后一个一个往下写(显得自信，厉害)</li>
<li>照片弄得好看一点</li>
<li>说明自己时间充足</li>
</ol>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
      <tags>
        <tag>简历</tag>
      </tags>
  </entry>
  <entry>
    <title>正规方程补充</title>
    <url>/2025/08/09/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E8%A1%A5%E5%85%85/</url>
    <content><![CDATA[<h2 id="正规方程在矩阵不可逆-奇异矩阵-的情况下的解决办法"><a href="#正规方程在矩阵不可逆-奇异矩阵-的情况下的解决办法" class="headerlink" title="正规方程在矩阵不可逆(奇异矩阵)的情况下的解决办法"></a>正规方程在矩阵不可逆(奇异矩阵)的情况下的解决办法</h2><p> 首先呢，这种情况极少数发生，而且即使发生了，如果用octave中的pinv，如果不可逆也能求出伪逆<br><img src="/img/ML/p1.png" alt=""></p>
<h3 id="为什么-X-TX-会出现不可逆的情况？"><a href="#为什么-X-TX-会出现不可逆的情况？" class="headerlink" title="为什么$X^TX$会出现不可逆的情况？"></a>为什么$X^TX$会出现不可逆的情况？</h3><ol>
<li>包含了多余的特征，比如一个以米为单位，另一个以英尺为单位，线性相关</li>
<li>运行的算法的特征太多了($m\le n$)<br>在此情况下，我们通常要删掉一些特征或者使用<strong>正则化</strong></li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>正规方程</tag>
      </tags>
  </entry>
  <entry>
    <title>科研重要知识点</title>
    <url>/2025/11/02/%E7%A7%91%E7%A0%94%E9%87%8D%E8%A6%81%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
    <content><![CDATA[<h3 id="何为断控"><a href="#何为断控" class="headerlink" title="何为断控"></a>何为断控</h3><p> 一个关键的新概念：“断控体”<br>原文： “…溶蚀作用较弱, 称为“断控体”…”<br>解析： 这是顺北油田区别于塔河油田等其他缝洞型油气藏的核心特征。<br>断溶体 (Fault-Karst Body): “断裂”和“溶蚀”共同作用，且溶蚀作用强。可以想象成先有裂缝，然后水沿着裂缝把岩石溶蚀成大大小小的洞穴。<br>断控体 (Fault-Controlled Body): “断裂”起绝对主导作用，溶蚀作用很弱。储集空间主要是由断裂运动直接产生的裂缝、破碎带构成的。<br>对您课题的意义： 您要建模的对象，其几何形态和内部结构更多地遵循构造物理学的规律，而不是化学溶蚀的规律。</p>
<h3 id="三大储油空间"><a href="#三大储油空间" class="headerlink" title="三大储油空间"></a>三大储油空间</h3><p>三大储油空间<br>原文： “…沿断裂带发育断裂面、类洞穴和杂乱体三大储集空间。”<br>解析： 这明确了储层不是一整块，而是由三种不同形态的地质体构成的复合体。<br>断裂面 (Fracture Planes): 类似“面”状的储集空间，是油气高速公路。<br>类洞穴 (Cave-like Bodies): 在断裂活动最强烈的地方形成的“团块”状破碎空间，是油气的巨型仓库。<br>杂乱体 (Disordered Bodies): 形态不规则的破碎区域。<br>对您课题的意义： 您的模型需要能够识别并分别刻画这三种不同形态的储集体。</p>
<h3 id="栅状充填结构"><a href="#栅状充填结构" class="headerlink" title="栅状充填结构"></a>栅状充填结构</h3><p>原文： “…断裂面与类洞穴内部…构造破碎区域被角砾充填, 形成角砾带, 与破碎程度弱的基岩带交替发育, 形成栅状充填结构特征…”<br>解析： 这是最核心、最关键的一句话，直接点明了储层内部的精细结构，也是传统建模方法最头疼的地方。<br>它说明储层内部不是空洞，也不是均匀的砂子填充。<br>它是一种有序的、交替出现的结构。<br>角砾带 (Breccia Belt): 被构造运动震碎的岩石碎块（角砾）堆积区。这里孔隙发育，是优质储层。<br>基岩带 (Bedrock Belt): 相对完整的、破碎程度弱的岩石。这里基本不储油，是非储层。<br>栅状结构 (Grille-like Structure): 这两种“带”像栅栏的木条和空隙一样交替排列，形成了油气的优势渗流通道。<br>对您课题的意义： 您的扩散模型的核心任务，就是要能够生成这种地质意义明确、结构特征清晰的“栅状结构”。 这将是您研究成果的最大亮点。</p>
<h3 id="最终的研究目标：定量化三维地质模型"><a href="#最终的研究目标：定量化三维地质模型" class="headerlink" title="最终的研究目标：定量化三维地质模型"></a>最终的研究目标：定量化三维地质模型</h3><p>原文： “为实现定量化表征…建立定量化的三维油气藏地质模型成为重点研究方向之一…”<br>解析： 这句话指出了最终的落脚点。地质研究不能只停留在定性的描述上，必须转化为定量的、可用于计算的三维数字模型。这个模型是后续进行储量计算、油井部署、开采方案模拟的唯一依据。<br>对您课题的意义： 您用扩散模型生成的，不是一张好看的图片，而是一个包含空间坐标和岩石类型（角砾带/基岩带）属性的三维数据体。</p>
<h3 id="主应力"><a href="#主应力" class="headerlink" title="主应力"></a>主应力</h3><p>主应力是描述物体（岩石）内部受力状态的最简化方式。</p>
<p>它由三个相互垂直、没有剪切力（纯挤压或拉伸）的应力（σ₁, σ₂, σ₃）组成。</p>
<p>主应力的大小和方向，决定了岩石将会如何破裂以及裂缝的走向。</p>
<p>岩石最容易沿着一个与最大主应力（σ₁）方向成大约 30 度角的方向发生剪切破裂。</p>
<p>在您的课题中，主应力是形成储层内部“栅状结构”的根本原因和总指挥。理解了主应力，才能理解栅状结构的成因和分布规律，也才能建立出符合地质规律的模型。</p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
  </entry>
  <entry>
    <title>科研竞赛规划</title>
    <url>/2025/11/20/%E7%A7%91%E7%A0%94%E7%AB%9E%E8%B5%9B%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<p>同学你好！非常理解你现在的处境。</p>
<p>首先，<strong>大二失去保研资格虽然遗憾，但绝对不是“末日”</strong>。相反，这在某种程度上是一种“解脱”。因为你不再需要为了每门课的绩点（GPA）去卷那些死记硬背的通识课或者你不喜欢的课，你可以把所有精力“梭哈”在你真正感兴趣且高回报的<strong>硬核技术</strong>和<strong>科研产出</strong>上。</p>
<p>对于计算机专业的学生，尤其是想要走科研路线（考研复试加分、申海外名校、进大厂算法岗）的同学来说，<strong>一篇顶会/顶刊论文 OR 一块高含金量的竞赛金牌 &gt;&gt;&gt; 0.5的绩点差距</strong>。</p>
<p>既然数据还要两个月才到，这 <strong>两个月既是“空窗期”，也是“黄金练兵期”</strong>。以下是我为你制定的“科研+竞赛”双线并行的冲刺建议：</p>
<h3 id="一、-竞赛篇：贵精不贵多，要与科研互补"><a href="#一、-竞赛篇：贵精不贵多，要与科研互补" class="headerlink" title="一、 竞赛篇：贵精不贵多，要与科研互补"></a>一、 竞赛篇：贵精不贵多，要与科研互补</h3><p>既然你的方向是<strong>扩散模型（AIGC/CV方向）</strong>，去搞ACM（算法竞赛）性价比就不高了（耗时太长，且与AI技能树不完全重合）。建议关注以下三类：</p>
<h4 id="1-必须要冲的：Kaggle-天池大数据竞赛（CV-生成赛道）"><a href="#1-必须要冲的：Kaggle-天池大数据竞赛（CV-生成赛道）" class="headerlink" title="1. 必须要冲的：Kaggle / 天池大数据竞赛（CV/生成赛道）"></a>1. 必须要冲的：Kaggle / 天池大数据竞赛（CV/生成赛道）</h4><ul>
<li><strong>为什么做：</strong> 这是最直接提升代码能力和工程能力的途径。你的师兄提到你“之前都是用AI实现的，没有自己动手编写”，这是科研的大忌。通过打这种比赛，你必须自己写Dataloader，自己调参，自己改模型结构。</li>
<li><strong>怎么做：</strong><ul>
<li>去 Kaggle 上找 <strong>Image Generation</strong> 或 <strong>Image Restoration</strong>（图像修复/超分）相关的过往比赛（Completed Competitions）。</li>
<li><strong>Action：</strong> 不要只看Notebook，尝试下载第一名（Top 1）的开源代码，在自己的电脑或服务器上跑通，然后试图理解他为什么这么改网络结构。</li>
<li><strong>关联性：</strong> 油气藏建模本质上可以看作是一种<strong>3D图像生成</strong>或者<strong>受限条件下的图像补全</strong>任务。Kaggle上的很多医学图像分割/生成比赛，和地质图像处理在数学原理上是高度相通的。</li>
</ul>
</li>
</ul>
<h4 id="2-时间点刚好的：美赛-MCM-ICM-2025年2月"><a href="#2-时间点刚好的：美赛-MCM-ICM-2025年2月" class="headerlink" title="2. 时间点刚好的：美赛 (MCM/ICM) - 2025年2月"></a>2. 时间点刚好的：美赛 (MCM/ICM) - 2025年2月</h4><ul>
<li><strong>为什么做：</strong> 美国大学生数学建模竞赛通常在春节前后（2月中旬）。</li>
<li><strong>优势：</strong> 它的含金量在国内考研复试和申请留学中都还不错（特别是O奖/F奖/M奖）。</li>
<li><strong>结合点：</strong> 既然你懂扩散模型和U-Net，你可以负责队伍里的<strong>编程/建模手</strong>。现在的数模比赛非常卷，用到深度学习模型（如LSTM预测、CNN分类、甚至生成模型做数据增强）的队伍往往能拿更好的奖。</li>
<li><strong>准备：</strong> 这两个月找两个靠谱队友（一个写论文厉害的，一个数学好的），专门练几套往年真题。</li>
</ul>
<h4 id="3-大创-挑战杯-互联网-（为项目做铺垫）"><a href="#3-大创-挑战杯-互联网-（为项目做铺垫）" class="headerlink" title="3. 大创/挑战杯/互联网+（为项目做铺垫）"></a>3. 大创/挑战杯/互联网+（为项目做铺垫）</h4><ul>
<li><strong>策略：</strong> 这个项目既然是跟博士师兄做的，大概率是一个比较大的课题。</li>
<li><strong>建议：</strong> 和师兄沟通，看这个项目是否可以包装一下，作为你<strong>大创项目（大学生创新创业训练计划）</strong> 的立项，或者准备明年的“挑战杯”。</li>
<li><strong>好处：</strong> 这种比赛不需要你现在就出结果，但是需要写本子（Project Proposal）。这两个月你可以通过写本子，强制自己去读完该领域所有的相关文献，把“故事”讲圆。</li>
</ul>
<hr>
<h3 id="二、-科研硬实力篇：解决“手生”的问题"><a href="#二、-科研硬实力篇：解决“手生”的问题" class="headerlink" title="二、 科研硬实力篇：解决“手生”的问题"></a>二、 科研硬实力篇：解决“手生”的问题</h3><p>师兄让你“先学会儿扩散模型，用现有数据训练”，这句话<strong>千万别当耳旁风</strong>。两个月时间，如果你只看不练，数据到了你还是抓瞎。</p>
<p>你的痛点是：<strong>理论懂，代码主要靠AI写，自己写不出来。</strong><br><strong>目标：</strong> 两个月后，能够不看教程，手搓一个简易版 Diffusion Model。</p>
<h4 id="1-代码复现计划（分三步走）"><a href="#1-代码复现计划（分三步走）" class="headerlink" title="1. 代码复现计划（分三步走）"></a>1. 代码复现计划（分三步走）</h4><ul>
<li><strong>第一周：PyTorch 基础夯实</strong><ul>
<li>别用 Keras，工业界和科研界现在基本都是 PyTorch。确保你会写 <code>Dataset</code>, <code>DataLoader</code>, <code>Module</code> 类，懂 <code>forward</code> 和 <code>backward</code> 的钩子。</li>
</ul>
</li>
<li><strong>第二周~第四周：复现 DDPM (Denoising Diffusion Probabilistic Models)</strong><ul>
<li>不要直接调包 <code>diffusers</code> 库！</li>
<li>去找一篇 Github 上 100-500 stars 左右的 <strong>DDPM implementation from scratch</strong>。</li>
<li><strong>任务：</strong> 逐行阅读，自己新建一个文件，一行行敲进去。理解每一个公式（比如重参数化技巧）对应哪一行代码。</li>
</ul>
</li>
<li><strong>第五周~第八周：复现 ControlNet</strong><ul>
<li>你的课题是“断控”，意味着有“控制条件”。ControlNet 正是解决这个问题的核心。</li>
<li>你需要搞懂：ControlNet 是怎么把额外的条件（比如地质层位的草图）注入到预训练的大模型里的？Zero Convolution 是什么？</li>
</ul>
</li>
</ul>
<h4 id="2-寻找替代数据集（Mock-Data）"><a href="#2-寻找替代数据集（Mock-Data）" class="headerlink" title="2. 寻找替代数据集（Mock Data）"></a>2. 寻找替代数据集（Mock Data）</h4><p>你没有地质数据，但模型不挑食。你可以找性质相似的公开数据集来练手：</p>
<ul>
<li><strong>医学影像数据（Medical Images）：</strong> CT或MRI扫描图。它们的纹理结构、孔隙特征和地下的岩石切片惊人的相似。用这些数据训练一个 Unet，做分割或生成，这套代码流程将来可以直接迁移到地质数据上。</li>
<li><strong>多孔介质数据集（Porous Media）：</strong> 网上有一些公开的数字岩心数据集，这直接对应你的专业。</li>
</ul>
<hr>
<h3 id="三、-针对你个人的战略建议"><a href="#三、-针对你个人的战略建议" class="headerlink" title="三、 针对你个人的战略建议"></a>三、 针对你个人的战略建议</h3><p>你大二，没有保研压力，这意味着你可以<strong>更冒险</strong>。</p>
<ol>
<li><p><strong>以“发Paper”为唯一核心指标</strong></p>
<ul>
<li>如果你想考研，复试时导师看到一个本科生有二作甚至一作的SCI/EI论文，基本双眼放光。</li>
<li>如果你想出国，论文更是硬通货。</li>
<li>所以，这两个月你要把扩散模型的相关论文（特别是应用于地质、遥感、医学领域的）看烂。不仅看模型，更要看<strong>他们怎么讲故事</strong>，怎么画图，怎么做对比实验。</li>
</ul>
</li>
<li><p><strong>英语不能丢</strong></p>
<ul>
<li>以后发论文要用英文，看文档要用英文。如果六级还没过，顺手刷了。如果过了，开始尝试直接看英文原版Paper，不要依赖翻译插件。</li>
</ul>
</li>
<li><p><strong>主动性（Proactive）</strong></p>
<ul>
<li><strong>不要等师兄给你分配任务。</strong></li>
<li>两个月后，如果你拿着一套<strong>已经跑通了医学CT数据生成</strong>的代码，并且告诉师兄：“师兄，我已经用类似的数据验证了ControlNet在处理这种纹理时的有效性，并且把Pipeline搭好了，只要地质数据一到，改一下DataLoader的路径就能跑。”</li>
<li>相信我，师兄和导师会把你当做核心培养对象的。</li>
</ul>
</li>
</ol>
<p><strong>总结一下这两个月的To-Do List：</strong></p>
<ol>
<li><strong>11月-12月：</strong> 彻底手写复现一遍 DDPM 和 ControlNet 的核心代码（用医学数据代替）。</li>
<li><strong>1月-2月：</strong> 组队准备美赛（MCM/ICM），争取拿个 M 奖以上。</li>
<li><strong>持续：</strong> 每周精读1-2篇相关方向的论文，做好笔记。</li>
</ol>
<p>加油！失去了保研的枷锁，你反而拥有了深耕科研的自由，这也许是你大学生涯的转折点。</p>
<p>这两个问题问得非常关键，这说明你已经准备好动手了。我们一个一个来解决，手把手教你怎么找资源。</p>
<h3 id="问题一：怎么在-Kaggle-上找相关的比赛？"><a href="#问题一：怎么在-Kaggle-上找相关的比赛？" class="headerlink" title="问题一：怎么在 Kaggle 上找相关的比赛？"></a>问题一：怎么在 Kaggle 上找相关的比赛？</h3><p>Kaggle 的界面全是英文，初学者容易迷路。你目前的课题是“断控缝洞型油气藏储层建模”，本质上涉及 <strong>图像生成（Generation）</strong>、<strong>图像修复（Inpainting/Restoration）</strong> 和 <strong>超分辨率（Super-Resolution）</strong>。</p>
<p>请按照以下步骤操作：</p>
<h4 id="1-搜索路径"><a href="#1-搜索路径" class="headerlink" title="1. 搜索路径"></a>1. 搜索路径</h4><ol>
<li>打开官网：<a href="https://www.kaggle.com/competitions">kaggle.com/competitions</a></li>
<li>在列表上方的筛选栏（Filters），将状态选为 <strong>“Completed”</strong>（已结束）。因为只有结束的比赛才有现成的代码（Notebooks/Kernels）和讨论区（Discussion）。</li>
<li>在搜索框输入关键词。</li>
</ol>
<h4 id="2-你应该搜哪些关键词？（按推荐程度排序）"><a href="#2-你应该搜哪些关键词？（按推荐程度排序）" class="headerlink" title="2. 你应该搜哪些关键词？（按推荐程度排序）"></a>2. 你应该搜哪些关键词？（按推荐程度排序）</h4><p>针对你的地质课题，建议搜索以下几类：</p>
<ul>
<li><p><strong>关键词 A：<code>Seismic</code> 或 <code>Salt</code> (强推)</strong></p>
<ul>
<li><strong>理由：</strong> 这是和你的课题最最相关的。虽然它们大多是“分割”任务，但<strong>数据的预处理、PyTorch DataLoader 的写法、地质图像的增强方法</strong>与你的课题是完全通用的。</li>
<li><strong>必看比赛：</strong> <strong>TGS Salt Identification Challenge</strong>。<ul>
<li>这个比赛是利用地震图像判断盐体位置。</li>
<li><strong>怎么学：</strong> 进去点 <strong>Code</strong> 标签，搜索 “U-Net”，看看别人怎么写 U-Net 的。你现在的课题虽然是扩散模型，但 ControlNet 的核心骨架也是 U-Net，这个基础必须打牢。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>关键词 B：<code>Inpainting</code> (图像修复)</strong></p>
<ul>
<li><strong>理由：</strong> “断控缝洞”建模有时候会被转化为“已知部分地质信息，补全未知部分”的任务，这在计算机视觉里就是 Inpainting。</li>
<li><strong>相关比赛：</strong> 虽然 Kaggle 上纯 Inpainting 的大比赛不多，但你可以搜 Code 里的 <strong>“Image Inpainting”</strong>。</li>
</ul>
</li>
<li><p><strong>关键词 C：<code>Generative</code> 或 <code>GAN</code></strong></p>
<ul>
<li><strong>理由：</strong> 在扩散模型火之前，大家用 GAN 做生成。</li>
<li><strong>必看比赛：</strong> <strong>Generative Dog Images</strong>。<ul>
<li>这是比较经典的生成比赛。虽然现在技术变了（从 GAN 变成了 Diffusion），但<strong>评估指标（比如 FID Score）</strong> 和 <strong>生成任务的训练流程</strong> 是有参考价值的。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="3-找到比赛后怎么看？（重点！）"><a href="#3-找到比赛后怎么看？（重点！）" class="headerlink" title="3. 找到比赛后怎么看？（重点！）"></a>3. 找到比赛后怎么看？（重点！）</h4><p>不要去看 Leaderboard（那是给神仙打架看的），你要看 <strong>“Code”</strong> 标签页：</p>
<ol>
<li>点击 <strong>Code</strong>。</li>
<li>Sort by（排序）选择 <strong>“Most Votes”</strong>（最高票）或者 <strong>“Best Score”</strong>（最高分）。</li>
<li>找标题里带 <strong>“PyTorch”</strong> 和 <strong>“Starter”</strong> 或 <strong>“Baseline”</strong> 的。</li>
<li><strong>作业：</strong> 找一个高票的 Notebook，下载下来，在你自己电脑上跑通。</li>
</ol>
<hr>
<h3 id="问题二：Github-上-100-500-stars-的-“DDPM-implementation-from-scratch”-是啥意思？"><a href="#问题二：Github-上-100-500-stars-的-“DDPM-implementation-from-scratch”-是啥意思？" class="headerlink" title="问题二：Github 上 100-500 stars 的 “DDPM implementation from scratch” 是啥意思？"></a>问题二：Github 上 100-500 stars 的 “DDPM implementation from scratch” 是啥意思？</h3><p>这里的 <strong>“Implementation” 指的是代码</strong>，不是论文（Paper）。<br><strong>“From scratch” 意思是“从零开始手写”</strong>，不依赖现成的高级库（如 <code>diffusers</code>）。</p>
<h4 id="1-为什么要找-100-500-stars-的？"><a href="#1-为什么要找-100-500-stars-的？" class="headerlink" title="1. 为什么要找 100-500 stars 的？"></a>1. 为什么要找 100-500 stars 的？</h4><p>这是一个<strong>“金发姑娘原则”</strong>（刚刚好）：</p>
<ul>
<li><strong>&lt; 50 stars：</strong> 可能是个人的烂尾工程，代码可能有错，没经过社区验证，坑多。</li>
<li><strong>&gt; 10,000 stars (比如 HuggingFace 的 diffusers)：</strong> 这种库封装得太好了，就像开自动挡的车。你调用一下 <code>pipeline()</code> 图就出来了，但你根本不知道底下发生了什么。你现在是为了学原理，为了将来能<strong>魔改</strong>模型，所以不能用这种“傻瓜式”库。</li>
<li><strong>100-500 stars：</strong> 通常是某个博士生、或者技术博主为了教学目的写的。<strong>代码结构清晰、文件少（可能就两三个 .py 文件）、注释多，非常适合学习原理。</strong></li>
</ul>
<h4 id="2-怎么找？"><a href="#2-怎么找？" class="headerlink" title="2. 怎么找？"></a>2. 怎么找？</h4><p>打开 <a href="https://github.com">github.com</a>，在搜索框输入：</p>
<ul>
<li><code>ddpm pytorch simple</code></li>
<li><code>diffusion model from scratch</code></li>
<li><code>minimal diffusion</code></li>
</ul>
<p>然后按 <strong>Stars</strong> 排序，往下翻一翻。</p>
<h4 id="3-直接给你推荐几个现成的（省去你找的时间）"><a href="#3-直接给你推荐几个现成的（省去你找的时间）" class="headerlink" title="3. 直接给你推荐几个现成的（省去你找的时间）"></a>3. 直接给你推荐几个现成的（省去你找的时间）</h4><p>为了让你这周就能上手，我给你挑了两个最适合你大二阶段学习的库：</p>
<ul>
<li><p><strong>推荐一：Dome272 / Diffusion-Models-pytorch</strong></p>
<ul>
<li><strong>特点：</strong> 这是一个 YouTube 大佬做的配套代码。</li>
<li><strong>为什么好：</strong> 代码极其精简，核心代码可能就几百行。它实现了 DDPM 的核心公式。</li>
<li><strong>学习方法：</strong> 如果你有梯子，去 YouTube 搜 “Dome272 Diffusion”，看着视频对着代码敲，一天就能悟道。</li>
</ul>
</li>
<li><p><strong>推荐二：labmlai / annotated_deep_learning_paper_implementations</strong></p>
<ul>
<li><strong>特点：</strong> 这是一个神库。它把经典论文（包括 DDPM）的代码写了一遍，并且<strong>在代码里写了巨详细的注释</strong>，告诉你这行代码对应论文里的哪个公式。</li>
<li><strong>怎么搜：</strong> 在这个库里找 <code>diffusion</code> 文件夹。</li>
</ul>
</li>
</ul>
<h4 id="你的行动计划："><a href="#你的行动计划：" class="headerlink" title="你的行动计划："></a>你的行动计划：</h4><ol>
<li><strong>下载代码：</strong> 把上面推荐的任意一个库 <code>git clone</code> 下来。</li>
<li><strong>读懂 <code>Forward</code>：</strong> 找到代码里加噪声的那一步（通常叫 <code>q_sample</code>），看它是怎么把一张清晰图片变成全是噪点的图的。</li>
<li><strong>读懂 <code>Reverse</code>：</strong> 找到 U-Net 模型定义的地方，看它输入是什么，输出是什么。</li>
<li><strong>自己跑：</strong> 用你的电脑（或者云服务器）跑一下训练，哪怕只训练 10 个 epoch，看着 Loss 下降，生成出模糊的图像，你的感觉就完全不一样了。</li>
</ol>
<p>去做吧！这两个月把这个“手写代码”搞定，比看 50 篇论文都管用。遇到代码看不懂的，直接把那段代码复制给 AI 让他给你解释。</p>
<p>这张截图里的资源非常棒！虽然都是7年前的代码（那是U-Net刚火的时候），但对于你学习<strong>网络结构</strong>和<strong>处理地质数据</strong>的逻辑来说，依然是教科书级别的。</p>
<p>针对你的需求（<strong>大二学生、做油气藏项目、为后续学Diffusion打基础</strong>），我建议你<strong>按顺序</strong>看以下这三个：</p>
<h3 id="第一顺位（必看）：Seismic-Data-Analysis-with-U-Net-（倒数第二项）"><a href="#第一顺位（必看）：Seismic-Data-Analysis-with-U-Net-（倒数第二项）" class="headerlink" title="第一顺位（必看）：Seismic Data Analysis with U-Net （倒数第二项）"></a>第一顺位（必看）：<code>Seismic Data Analysis with U-Net</code> （倒数第二项）</h3><ul>
<li><strong>理由：</strong><ul>
<li><strong>专业对口性 MAX：</strong> 看到标题里的 <strong>“Seismic”（地震数据）</strong> 了吗？这跟你要做的“断控缝洞型油气藏”是亲兄弟。</li>
<li><strong>核心价值：</strong> 这个 Notebook 会教你如何处理这种<strong>灰度、高噪、纹理复杂</strong>的地质勘探图像。虽然它的票数（182）不是最高，但它对你的<strong>科研课题</strong>（Project）最有参考价值。你需要看看他是怎么做数据预处理（Preprocessing）和归一化（Normalization）的。</li>
</ul>
</li>
</ul>
<h3 id="第二顺位（技术进阶）：U-net-with-simple-ResNet-Blocks-v2-（第五项）"><a href="#第二顺位（技术进阶）：U-net-with-simple-ResNet-Blocks-v2-（第五项）" class="headerlink" title="第二顺位（技术进阶）：U-net with simple ResNet Blocks v2 （第五项）"></a>第二顺位（技术进阶）：<code>U-net with simple ResNet Blocks v2</code> （第五项）</h3><ul>
<li><strong>理由：</strong><ul>
<li><strong>为 Diffusion 铺路：</strong> 这一点非常关键。标准的 U-Net 是比较简单的，但<strong>现在的扩散模型（比如 Stable Diffusion）里的 U-Net，内部其实塞满了 ResNet Blocks（残差块）</strong>。</li>
<li><strong>学习点：</strong> 看这个代码可以帮你理解：怎么把 ResNet 的结构嵌入到 U-Net 的 Encoder 和 Decoder 里去。你看懂了这个，再去看 DDPM 或 ControlNet 的源码时，就不会被那些复杂的 Block 吓到了。</li>
</ul>
</li>
</ul>
<h3 id="第三顺位（兜底参考）：U-net-dropout-augmentation-stratification-（第一项）"><a href="#第三顺位（兜底参考）：U-net-dropout-augmentation-stratification-（第一项）" class="headerlink" title="第三顺位（兜底参考）：U-net, dropout, augmentation, stratification （第一项）"></a>第三顺位（兜底参考）：<code>U-net, dropout, augmentation, stratification</code> （第一项）</h3><ul>
<li><strong>理由：</strong><ul>
<li><strong>高票即正义：</strong> 1143 票的金牌 Notebook，代码质量和注释通常是最好的。</li>
<li><strong>学习点：</strong> 如果前面两个你看不太懂，或者报错跑不通，就回来跑这个。它里面提到的 <strong>Augmentation（数据增强）</strong> 对你也很重要——因为地质数据通常很少，你需要学会怎么把一张图“变”成十张图来训练（旋转、翻转、加噪）。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="💡-怎么“吃透”这几个代码？（操作指南）"><a href="#💡-怎么“吃透”这几个代码？（操作指南）" class="headerlink" title="💡 怎么“吃透”这几个代码？（操作指南）"></a>💡 怎么“吃透”这几个代码？（操作指南）</h3><p>既然你大二，可能 PyTorch 还没那么熟，不要只是点开看，请执行以下操作：</p>
<ol>
<li><p><strong>关注 <code>class UNet(nn.Module)</code>：</strong></p>
<ul>
<li>找到定义网络结构的这一段。</li>
<li>拿笔在纸上画一下它的 <code>forward</code> 流程：输入图片的尺寸是多少（比如 128x128）？经过一层卷积后变成了多少？最后输出是多少？</li>
<li><strong>重点：</strong> 搞懂 <code>torch.cat</code>（拼接）操作是在哪里发生的，这是 U-Net “Skip Connection” 的灵魂。</li>
</ul>
</li>
<li><p><strong>关注 <code>Dataset</code> 类：</strong></p>
<ul>
<li>看他是怎么把图片读进来的。</li>
<li>你的师兄说数据还有两个月才到，到时候数据到了，你只需要把这个 Notebook 里的<strong>文件路径</strong>改一改，把<strong>读取方式</strong>微调一下，你的项目雏形就有了！</li>
</ul>
</li>
</ol>
<p><strong>建议：</strong> 先点开 <strong><code>Seismic Data Analysis with U-Net</code></strong>，把它的代码下载下来（点击右上角的 “Copy and Edit” 或者下载 .ipynb 文件），在本地跑通它！有问题随时问我。</p>
]]></content>
      <categories>
        <category>科研</category>
        <category>竞赛</category>
      </categories>
  </entry>
  <entry>
    <title>缝洞型命名原因</title>
    <url>/2025/11/01/%E7%BC%9D%E6%B4%9E%E5%9E%8B%E7%A2%B3%E9%85%B8%E7%9B%90%E5%B2%A9%E5%82%A8%E9%9B%86%E4%BD%93%E5%91%BD%E5%90%8D%E5%8E%9F%E5%9B%A0/</url>
    <content><![CDATA[<p>你这个问题问得太好了！我们一直在说这个长长的、拗口的专业名词，但把它拆解开，理解每个词的含义，正是理解你研究对象本质的第一步。</p>
<p>让我们像“庖丁解牛”一样，把“<strong>缝洞型碳酸盐岩储集体</strong>”这个词彻底分解开。</p>
<hr>
<h3 id="1-储集体-Reservoir-Body"><a href="#1-储集体-Reservoir-Body" class="headerlink" title="1. 储集体 (Reservoir Body)"></a><strong>1. 储集体 (Reservoir Body)</strong></h3><p>我们从后往前说，这个我们刚刚讨论过。</p>
<ul>
<li><strong>含义</strong>: 地下那些<strong>真正能大量储存油气</strong>的“<strong>有效空间</strong>”或“<strong>甜点</strong>”。</li>
<li><strong>比喻</strong>: 停车场里<strong>真正能停车的“车位”</strong>。</li>
</ul>
<hr>
<h3 id="2-碳酸盐岩-Carbonate-Rock"><a href="#2-碳酸盐岩-Carbonate-Rock" class="headerlink" title="2. 碳酸盐岩 (Carbonate Rock)"></a><strong>2. 碳酸盐岩 (Carbonate Rock)</strong></h3><ul>
<li><strong>这是什么？</strong> 这指的是构成这个储集体的<strong>岩石类型</strong>。</li>
<li><strong>含义</strong>:<ul>
<li>“<strong>碳酸盐</strong>”：化学成分主要是<strong>碳酸钙 (CaCO₃)</strong>（石灰岩的主要成分）或<strong>碳酸镁钙 (CaMg(CO₃)₂) </strong>（白云岩的主要成分）。</li>
<li><strong>“岩”</strong>: 就是石头。</li>
</ul>
</li>
<li><strong>它是怎么形成的？</strong><ul>
<li>与我们常见的由泥沙（硅酸盐）构成的砂岩、泥岩不同，碳酸盐岩主要是由<strong>生物</strong>的<strong>遗骸</strong>（比如贝壳、珊瑚、藻类）和化学沉淀，在古代的<strong>温暖、清澈的浅海</strong>环境中形成的。</li>
<li>想象一下现在的巴哈马群岛、澳大利亚大堡礁，那里正在形成的，就是未来的碳酸盐岩。</li>
</ul>
</li>
<li><strong>为什么它很重要？</strong><ul>
<li>碳酸盐岩有一个<strong>极其重要</strong>的特性：它<strong>不耐酸</strong>！它很容易被含有二氧化碳的<strong>酸性水</strong>（比如雨水、地下水）<strong>溶解</strong>。</li>
<li><code>CaCO₃ + H₂O + CO₂ &lt;=&gt; Ca(HCO₃)₂</code> (可溶的碳酸氢钙)</li>
<li>这个化学特性，是形成我们下面要讲的“缝洞型”储集体的<strong>物质基础</strong>。</li>
</ul>
</li>
</ul>
<p><strong>简单说</strong>: <strong>“碳酸盐岩”</strong>告诉了我们这个储层是由什么样的<strong>“建筑材料”</strong>（主要是石灰岩和白云岩）构成的。</p>
<hr>
<h3 id="3-缝洞型-Fracture-Cavity-Type"><a href="#3-缝洞型-Fracture-Cavity-Type" class="headerlink" title="3. 缝洞型 (Fracture-Cavity Type)"></a><strong>3. 缝洞型 (Fracture-Cavity Type)</strong></h3><p>这个词是整个名称的<strong>核心</strong>，它描述了这个储集体<strong>内部储集空间的“形态”</strong>。它由两个字组成：</p>
<h4 id="a-“缝-Fracture-”-裂缝"><a href="#a-“缝-Fracture-”-裂缝" class="headerlink" title="a. “缝 (Fracture)” - 裂缝"></a><strong>a. “缝 (Fracture)” - 裂缝</strong></h4><ul>
<li><strong>含义</strong>: 指的是岩石因为受到<strong>地质构造运动</strong>（比如地壳的挤压、拉伸）的作用而产生的<strong>破裂面</strong>。</li>
<li><strong>它是怎么形成的？</strong><ul>
<li>想象一块巨大的岩石，被两边的地块用力<strong>挤压或拉扯</strong>，当应力超过了岩石的强度极限，它就会“<strong>咔嚓</strong>”一声裂开。</li>
<li>在你的研究区（顺北），这些裂缝主要是由“<strong>走滑断裂带</strong>”的强烈构造运动产生的。</li>
</ul>
</li>
<li><strong>形态</strong>: 可以是只有几毫米宽、延伸几米的<strong>微小裂缝</strong>，也可以是几米甚至几十米宽、延伸几公里的<strong>巨型断裂破碎带</strong>。</li>
<li><strong>作用</strong>:<ul>
<li>它们本身可以储存一部分油气。</li>
<li>更重要的是，它们是连接各个储集空间的“<strong>高速公路</strong>”，是油气<strong>运移和产出</strong>的主要<strong>通道</strong>。</li>
</ul>
</li>
</ul>
<h4 id="b-“洞-Cavity-Vug-”-洞穴"><a href="#b-“洞-Cavity-Vug-”-洞穴" class="headerlink" title="b. “洞 (Cavity / Vug)” - 洞穴"></a><strong>b. “洞 (Cavity / Vug)” - 洞穴</strong></h4><ul>
<li><strong>含义</strong>: 指的是岩石内部被<strong>溶蚀</strong>后形成的、各种尺寸的<strong>孔洞</strong>。</li>
<li><strong>它是怎么形成的？</strong><ul>
<li>这就是<strong>碳酸盐岩</strong>那个“不耐酸”的特性发挥作用的时候了！</li>
<li>当地下的酸性水（通常是大气降水渗入地下形成）沿着<strong>裂缝（“缝”）</strong>流动时，它会不断地<strong>溶解</strong>裂缝两侧的<strong>碳酸盐岩</strong>。</li>
<li>天长日久，小小的裂缝就会被逐渐溶蚀、扩大，最终形成大大小小的<strong>洞穴</strong>。</li>
<li>这个过程，地质学上叫做“<strong>岩溶作用 (Karstification)</strong>”，和我们看到的桂林山水、地下溶洞的成因是一样的。</li>
</ul>
</li>
<li><strong>形态</strong>: 小的像米粒（溶孔），大的可以像房子甚至一个体育场（巨型溶洞）。</li>
<li><strong>作用</strong>: 这是<strong>最主要、最大规模</strong>的油气<strong>储存空间</strong>。</li>
</ul>
<p><strong>“缝洞型”这个词，就把这两个核心概念完美地结合在了一起。</strong> 它告诉你：</p>
<ul>
<li>这个储集体的形成，是<strong>“构造运动”（形成“缝”）</strong>和<strong>“化学溶蚀”（形成“洞”）</strong>两种地质作用<strong>共同</strong>的产物。</li>
<li>“<strong>缝</strong>”是“<strong>洞</strong>”形成的基础和通道。</li>
<li>“<strong>缝</strong>”和“<strong>洞</strong>”相互连接，构成了一个极其复杂的、既能储油又能输油的<strong>三维网络系统</strong>。</li>
</ul>
<hr>
<h3 id="总结：一句话概括"><a href="#总结：一句话概括" class="headerlink" title="总结：一句话概括"></a><strong>总结：一句话概括</strong></h3><p>“<strong>缝洞型碳酸盐岩储集体</strong>” 这个长长的名词，其实是在讲一个故事：</p>
<p>“在古代的浅海里，由<strong>生物遗骸</strong>堆积形成的<strong>碳酸盐岩</strong>地层 (碳酸盐岩)，后来因为<strong>地壳运动</strong>而被震裂，产生了大量的<strong>裂缝</strong> (缝)，之后，酸性的<strong>地下水</strong>又沿着这些裂缝不断地<strong>溶解</strong>岩石，形成了大大小小的<strong>洞穴</strong> (洞)，这些相互连通的缝和洞，最终成为了我们今天开采油气的<strong>主要空间</strong> (储集体)。”</p>
]]></content>
      <categories>
        <category>科研</category>
      </categories>
  </entry>
  <entry>
    <title>知识点积累</title>
    <url>/2025/08/17/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%BD%9C%E4%B8%9A/</url>
    <content><![CDATA[<p><img src="/img/ML/p36.png" alt=""></p>
<h3 id="1-线性回归与逻辑回归里的假设函数和代价函数向量化是咋实现的啊┭┮﹏┭┮"><a href="#1-线性回归与逻辑回归里的假设函数和代价函数向量化是咋实现的啊┭┮﹏┭┮" class="headerlink" title="1.线性回归与逻辑回归里的假设函数和代价函数向量化是咋实现的啊┭┮﹏┭┮"></a>1.线性回归与逻辑回归里的假设函数和代价函数向量化是咋实现的啊┭┮﹏┭┮</h3><h3 id="2-那逻辑回归中梯度下降函数的向量化呢"><a href="#2-那逻辑回归中梯度下降函数的向量化呢" class="headerlink" title="2.那逻辑回归中梯度下降函数的向量化呢"></a>2.那逻辑回归中梯度下降函数的向量化呢</h3><h3 id="3-fig-ax-plt-subplots-后面的式子看不懂"><a href="#3-fig-ax-plt-subplots-后面的式子看不懂" class="headerlink" title="3.fig,ax=plt.subplots()后面的式子看不懂"></a>3.fig,ax=plt.subplots()后面的式子看不懂</h3><h3 id="4-data为啥可以用两个"><a href="#4-data为啥可以用两个" class="headerlink" title="4.data为啥可以用两个[]"></a>4.data为啥可以用两个[]</h3><p><img src="/img/ML/p37.png" alt=""></p>
<p><img src="/img/ML/p38.png" alt=""></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>作业</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络</title>
    <url>/2025/08/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="part-1-神经网络的基本概念"><a href="#part-1-神经网络的基本概念" class="headerlink" title="part 1 神经网络的基本概念"></a>part 1 神经网络的基本概念</h3><p>为什么已经有了线性回归和logistic回归算法，还需要研究神经网络？</p>
<p><strong>因为它在学习复杂的非线性假设上被证明是一种好得多的算法，即使输入特征空间或n很大，也能轻松搞定</strong></p>
<p><img src="/img/ML/p23.png" alt=""><br>$x_0$被称为偏置单元或偏置神经元</p>
<p>带有sigmoid或者logistic<strong>激活函数</strong>的人工神经元</p>
<p>激活函数是指非线性函数$g(z)$，$g(z)=\frac{1}{1+e^{-z}}$</p>
<p>$\theta$可以称为模型的参数，但在大多数文献中叫模型的<strong>权重</strong></p>
<p><img src="/img/ML/p24.png" alt=""><br>神经网络其实就是一组神经元连接在一起的集合</p>
<p>任何非输入层和非输出层，都是隐藏层</p>
<p><img src="/img/ML/p25.png" alt=""><br>$a^{(j)}_i$表示第j层第i个神经元或单元的<strong>激活项</strong>，激活项是指由一个具体神经元计算并输出的值</p>
<p>此外，我们的神经网络被这些矩阵参数化——$\theta^{(j)}$，即权重矩阵。它控制从某一层到下一层的映射</p>
<p>在此图，$\theta^{(1)}$就是控制着从三个输入单元到三个隐藏单元的映射的参数矩阵，它是一个3*4矩阵<br><img src="/img/ML/p26.png" alt=""></p>
<p>最后在输出层我们还有一个单元，它计算h(x),也可以写成$a^{(3)}_1$</p>
<p><strong>总结一下</strong>，我们讲解了这样一张图是如何定义一个人工神经网络的。其中的神经网络定义了函数h从输入x到输出y的映射。这些假设被参数化，记作$\Theta$，这样一来，改变$\Theta$就能得到不同的假设</p>
<h3 id="part-2-神经网络的具体实现"><a href="#part-2-神经网络的具体实现" class="headerlink" title="part 2 神经网络的具体实现"></a>part 2 神经网络的具体实现</h3><p>a和z的上标表示这些值与哪一层有关</p>
<p>这些z的值都是神经元输入值的加权线性组合(所谓加权，其实就是这些系数不是随便取，而是有特殊意义，常常被叫做“权重”。权重大，就影响大，权重小，就影响小<br>。。。。。哎呀就是x的系数是权重啦)</p>
<p><strong>前向传播：我们从输入单元的激活项开始，然后向前传播给隐藏层，计算隐藏层的激活项，然后我们继续前向传播，并计算输出层的激活项。这个依次计算激活项，从输入层到隐藏层再到输出层的过程叫前向传播</strong></p>
<p><img src="/img/ML/p27.png" alt=""></p>
<p>这张图推导出的是向前传播的向量化实现方法</p>
<p><a href="https://www.bilibili.com/video/BV164411b7dx?t=200.1&amp;p=46">图没看懂就点这个，精准空降</a></p>
<h3 id="part-3-前向传播如何帮助我们了解神经网络的作用和它为什么能帮助我们学习有趣的非线性假设函数"><a href="#part-3-前向传播如何帮助我们了解神经网络的作用和它为什么能帮助我们学习有趣的非线性假设函数" class="headerlink" title="part 3 前向传播如何帮助我们了解神经网络的作用和它为什么能帮助我们学习有趣的非线性假设函数"></a>part 3 前向传播如何帮助我们了解神经网络的作用和它为什么能帮助我们学习有趣的非线性假设函数</h3><h4 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h4><p><img src="/img/ML/p28.png" alt=""></p>
<p>把x遮住，你会发现它的形式与逻辑回归一样。区别在于，输入项由x变成a，这就带来了极大地可操作性，因为由x到a的参数可以自己设计。</p>
<p><img src="/img/ML/p29.png" alt=""><br>神经网络中神经元的连接方式被称为神经网络的<strong>架构</strong></p>
<h4 id="具体例子"><a href="#具体例子" class="headerlink" title="具体例子"></a>具体例子</h4><p><img src="/img/ML/p30.png" alt=""><br>我们想做的是学习一个非线性的判断边界来区分正样本和负样本</p>
<p>具体来说，我们需要计算目标函数，如图<br><img src="/img/ML/p31.png" alt=""></p>
<p>我们如何构建一个神经网络来拟合这样的训练集(XNOR)呢</p>
<h5 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h5><p>我们先从能够拟合AND运算的网络入手(只含单个神经元)<br><img src="/img/ML/p32.png" alt=""></p>
<p>首先我们要加一个偏置单元(+1单元)，然后对偏重/参数进行赋值</p>
<p>通过写出真值表，我们就能弄清楚逻辑函数的取值是怎样的</p>
<p>逻辑或同理<br><img src="/img/ML/p33.png" alt=""></p>
<h5 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h5><p>实现逻辑非运算，大体思想就是在预期得到非结果的变量前面放一个很大的<strong>负权重</strong><br><img src="/img/ML/p34.png" alt=""></p>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>为啥神经网络可以计算这种复杂的函数呢？</p>
<p>我们的输入都放在输入层，然后在中间放一个隐藏层，用来计算一些关于输入的略微复杂的功能，然后再继续增加一层，用于计算一个更复杂的非线性函数</p>
<p>当神经网络有很多层时，在第二层中有一些关于输入的相对简单的函数，第三层又在此基础上计算更加复杂的方程，再往后一层，计算的函数越来越复杂</p>
<h3 id="part-4-如何利用神经网络解决多类别分类问题"><a href="#part-4-如何利用神经网络解决多类别分类问题" class="headerlink" title="part 4 如何利用神经网络解决多类别分类问题"></a>part 4 如何利用神经网络解决多类别分类问题</h3><p>采用的方法本质上是<strong>一对多法的拓展</strong></p>
<p><img src="/img/ML/p34.png" alt=""><br>我们要做的是建立一个有四个输出单元的神经网络，现在神经网络的输出将是一个四维的向量</p>
<p>这其实就像一对多法，现在可以说我们有4个逻辑回归分类器，它们每一个都将识别图片中的物体是否是它那一种</p>
<p>$y^{（i）}$</p>
<p>每一个训练样本都由($x^{（i）}，y^{（i）}$)组成。其中$x^{（i）}$就是四种物体其中一种的图像，而$y^{（i）}$就是这些向量中的一个</p>
<p>我们希望找到一个办法，让神经网络的输出值$h_\Theta(x^{(i)})$约等于$y^{(i)}$，并且它俩在该例子中都是四维向量，分别代表四种不同的类别</p>
<h3 id="part-5-神经网络的代价函数"><a href="#part-5-神经网络的代价函数" class="headerlink" title="part 5 神经网络的代价函数"></a>part 5 神经网络的代价函数</h3><p><img src="/img/ML/p39.png" alt=""><br><img src="/img/ML/p40.png" alt=""><br>正则化那一项不包括i=0，因为i=0的都是偏置项的参数</p>
<h3 id="part-6-反向传播算法"><a href="#part-6-反向传播算法" class="headerlink" title="part 6 反向传播算法"></a>part 6 反向传播算法</h3><p><img src="/img/ML/p41.png" alt=""><br>我们需要计算need那里面的两个，j我们可以通过上面的公式来求，重点是计算偏导项(参数的偏导数)</p>
<p><img src="/img/ML/p42.png" alt=""><br>通过把前向传播算法向量化，我们得到了神经网络结构里的每一个神经元的<strong>激活值</strong></p>
<p>接下来，为了计算偏导项，我们采用<strong>反向传播算法</strong></p>
<p><img src="/img/ML/p43.png" alt=""><br>反向传播算法从直观上说，就是对每一个结点计算这样一项$\delta^{(l)}_j$,代表第l层的第j个结点的误差</p>
<p>注意理解$g’$，指的是对激活函数g在输入值为$z(3)$的时候所求的导数。得到的结果就是右边那个a一堆</p>
<p>没有$\delta^{(0)}$，因为是输入层，不存在误差</p>
<p><img src="/img/ML/p44.png" alt=""><br>$\Delta^{(l)}$是一个矩阵，不是一个列向量！！！你自己想一下嘛。对了，它叫梯度累加器</p>
<p>最后求D的时候除以m是因为求平均值</p>
<p><img src="/img/ML/p45.png" alt=""><br><img src="/img/ML/p46.png" alt=""></p>
<p>对比一下前向传播和反向传播，其实非常相似。要注意的是，反向传播求$\delta$时注意不要加上偏置单元，没有用，永远都是1</p>
<h4 id="展开参数"><a href="#展开参数" class="headerlink" title="展开参数"></a>展开参数</h4><p>高级优化算法假定$\theta$是参数向量，假定梯度值也为向量。</p>
<p>然而在神经网络里，参数是矩阵，梯度值也是矩阵，因此要通过thetaVec转化为长向量<br><img src="/img/ML/p47.png" alt=""><br>过程详情图</p>
<h4 id="梯度检测"><a href="#梯度检测" class="headerlink" title="梯度检测"></a>梯度检测</h4><p><img src="/img/ML/p48.png" alt=""><br><img src="/img/ML/p49.png" alt=""><br><img src="/img/ML/p50.png" alt=""><br><img src="/img/ML/p51.png" alt=""><br>在训练之前要关掉梯度检验，不然会严重拖慢运行速度</p>
<h4 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h4><p><img src="/img/ML/p52.png" alt=""></p>
<p>逻辑回归当中我们是把参数全初始化为0，但是在神经网络中如果把参数全部初始化为0，那么不管进行多少次迭代，参数都相等，所有的隐藏单元都以相同的函数作为输入。这是一种高度冗余的现象，阻止了神经网络去学习任何有趣的东西。</p>
<p>因此，在神经网络问题当中有必要进行<strong>随机初始化</strong></p>
<p><img src="/img/ML/p53.png" alt=""><br>我们刚才碰到的这种问题叫做<strong>对称权重问题</strong>，也就是所有的权重都是一样的，随机初始化用来解决这一问题</p>
<p>对每一个$\theta$值，我们将其初始化为一个范围在$[-\epsilon,+\epsilon]$的随机值</p>
<p>rand(10,11)会生成一个值都介于0到1之间的10x11随机矩阵</p>
<p>总而言之，为了训练神经网络，应首先要将权重随机初始化为一个接近0的范围在$[-\epsilon,+\epsilon]$的数，然后进行反向传播，再进行梯度检验，最后使用梯度下降或者其他高级优化算法来最小化代价函数J</p>
<p>上面的表述有些问题，看看ai studio给的正确解答</p>
<p>您这个问题问得实在是太好了！您的总结已经非常接近正确答案了，但其中有一个非常关键的、也是最容易让初学者混淆的<strong>顺序问题</strong>。</p>
<p>您问的“那啥时候前向传播啊”，正好就打在了这个问题的七寸上。</p>
<p>您的总结顺序是：<code>初始化 → 反向传播 → 梯度检验 → 梯度下降</code>。<br><strong>这个顺序是不对的。</strong></p>
<p>让我们来梳理一下正确、清晰的完整流程。这个流程分为两个阶段：</p>
<ol>
<li><strong>开发与检验阶段 (一次性工作)</strong></li>
<li><strong>训练阶段 (成千上万次循环)</strong></li>
</ol>
<hr>
<h3 id="正确的流程"><a href="#正确的流程" class="headerlink" title="正确的流程"></a>正确的流程</h3><h4 id="阶段一：开发与检验-在你写代码时，只做一次"><a href="#阶段一：开发与检验-在你写代码时，只做一次" class="headerlink" title="阶段一：开发与检验 (在你写代码时，只做一次)"></a>阶段一：开发与检验 (在你写代码时，只做一次)</h4><p>这个阶段的目标是：<strong>确保你写的反向传播算法是100%正确的。</strong></p>
<ol>
<li><p><strong>编码实现</strong>：</p>
<ul>
<li>实现<strong>前向传播</strong>函数：输入数据和权重，输出预测值和成本 $J$。</li>
<li>实现<strong>反向传播</strong>函数：利用前向传播过程中的中间结果，计算出梯度矩阵 $D^{(1)}, D^{(2)}, …$。</li>
</ul>
</li>
<li><p><strong>进行梯度检验 (Gradient Checking)</strong>：</p>
<ul>
<li><strong>目的</strong>：验证你写的反向传播对不对。</li>
<li><strong>做法</strong>：<br>a. 使用我们之前讨论的<strong>数值近似方法</strong>（那个“向前走一步，向后走一步”的慢方法）计算出一个“标准答案”梯度 <code>grad_numerical</code>。<br>b. 调用你写的<strong>反向传播函数</strong>，得到一个“你的答案”梯度 <code>grad_backprop</code>。<br>c. <strong>对比</strong>这两个梯度向量。如果它们几乎完全相等，恭喜你，你的代码写对了！</li>
</ul>
</li>
<li><p><strong>关闭梯度检验</strong>：</p>
<ul>
<li>一旦你确认了你的反向传播代码是正确的，<strong>就必须把梯度检验功能关掉！</strong> 否则，你的训练会慢到无法忍受。</li>
</ul>
</li>
</ol>
<h4 id="阶段二：训练-在你确认代码无误后，反复执行"><a href="#阶段二：训练-在你确认代码无误后，反复执行" class="headerlink" title="阶段二：训练 (在你确认代码无误后，反复执行)"></a>阶段二：训练 (在你确认代码无误后，反复执行)</h4><p>这个阶段的目标是：<strong>利用你已经验证过的代码，真正地去学习和优化参数。</strong> 这就是梯度下降（或高级优化算法）发挥作用的地方，它是一个<strong>循环</strong>。</p>
<p><strong>现在，我们来回答您最核心的问题：“啥时候前向传播？”</strong></p>
<ol>
<li><p><strong>第0步：初始化</strong></p>
<ul>
<li>将权重 $\Theta$ 随机初始化为接近0的小数。<strong>(这一步只在循环开始前做一次)</strong></li>
</ul>
</li>
<li><p><strong>开始训练循环 (例如，循环1000次)</strong></p>
<ul>
<li><p><strong>第1步：前向传播 (Forward Propagation) - 答案在这里！</strong></p>
<ul>
<li><strong>目的</strong>：根据<strong>当前</strong>的权重 $\Theta$，计算出网络对训练数据的预测值。</li>
<li><strong>过程</strong>：将一批训练数据输入网络，从第一层开始，逐层向后计算，直到得到最后一层的输出。</li>
<li><strong>这是每一次迭代中，必须做的第一件事！</strong> 就像下山前，你必须先知道自己当前所在位置的海拔。</li>
</ul>
</li>
<li><p><strong>第2步：计算代价函数 (Cost Function)</strong></p>
<ul>
<li><strong>目的</strong>：评估当前预测的好坏。</li>
<li><strong>过程</strong>：将<strong>前向传播</strong>得到的预测值与真实标签进行比较，计算出成本 $J(\Theta)$。</li>
</ul>
</li>
<li><p><strong>第3步：反向传播 (Backpropagation)</strong></p>
<ul>
<li><strong>目的</strong>：计算梯度，即“下山地图”。</li>
<li><strong>过程</strong>：从刚刚计算出的成本（误差）出发，从后向前逐层计算代价函数对<strong>每一层权重</strong>的偏导数（梯度）。<strong>这一步严重依赖前向传播过程中产生的中间值</strong> (比如每一层的激活值 $a^{(l)}$ 和加权输入 $z^{(l)}$)。</li>
</ul>
</li>
<li><p><strong>第4步：更新权重 (Weight Update)</strong></p>
<ul>
<li><strong>目的</strong>：朝着山谷最低点迈出一步。</li>
<li><strong>过程</strong>：使用<strong>梯度下降</strong>或<strong>高级优化算法</strong>，根据<strong>反向传播</strong>计算出的梯度，来更新权重 $\Theta$。</li>
<li>例如，梯度下降：$\Theta := \Theta - \alpha \cdot (\text{梯度})$</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>回到第1步，开始下一次循环</strong></p>
<ul>
<li>用<strong>刚刚更新过</strong>的权重，重复进行“前向传播 → 计算代价 → 反向传播 → 更新权重”这个过程，直到成本不再下降或达到设定的迭代次数。</li>
</ul>
</li>
</ol>
<h3 id="总结与修正"><a href="#总结与修正" class="headerlink" title="总结与修正"></a>总结与修正</h3><p>所以，正确的流程应该是这样的：</p>
<ol>
<li><p><strong>一次性工作</strong>：</p>
<ul>
<li><code>初始化权重</code></li>
<li><code>(可选，仅在开发时) 运行梯度检验，确认代码无误后关闭</code></li>
</ul>
</li>
<li><p><strong>核心训练循环 (反复执行)</strong>：</p>
<ul>
<li><code>For i = 1 to N:</code><ul>
<li><strong><code>前向传播</code></strong> (计算预测和成本)</li>
<li><strong><code>反向传播</code></strong> (计算梯度)</li>
<li><strong><code>更新权重</code></strong> (使用梯度下降等优化算法)</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>所以，前向传播不是在反向传播之前或之后的一次性步骤，而是</strong>在每一次训练迭代中，都必须执行的第一步<strong>。它们是“你唱我和”的搭档，在每一次迭代中都紧密地捆绑在一起：</strong>前向传播算结果，反向传播算原因，优化器根据原因去修正。**</p>
<h3 id="part-7-神经网络算法总体实现过程"><a href="#part-7-神经网络算法总体实现过程" class="headerlink" title="part 7 神经网络算法总体实现过程"></a>part 7 神经网络算法总体实现过程</h3><ol>
<li><p>选择网络架构</p>
<p>比如每一层有多少个影藏单元，以及有多少个隐藏层</p>
<p>那么该如何选择呢？</p>
<p>首先我们由训练集就可以知道输入单元的个数了，输出单元的数量同理<br><img src="/img/ML/p54.png" alt=""><br><strong>注意</strong>，如果你的多元分类问题y的取值范围是在1到10之间，记得把输出y重新写成向量的形式，如图，我们把第一类重新写成这种形式的向量，第二个也是</p>
<p>如果其中一个样本被分到第五类，也就是y=5，那么在神经网络中就不能直接用数值5来表达，而是用一个向量来表达，这个向量的第五个位置值是1，其他都是0</p>
</li>
</ol>
<p>   <strong>那么对于隐藏层的数目和隐藏单元的个数呢?</strong></p>
<p>   一个合理的默认选项是只使用单个隐藏层，如果使用的不止一个的话，同样也有一个合理的默认选项，那就是每一个隐藏层通常都应有相同的单元数</p>
<p>   通常情况下，隐藏单元越多越好</p>
<ol>
<li><p>训练神经网络需要实现的步骤</p>
<ol>
<li>构建神经网络然后随机初始化权重</li>
<li>执行前向传播算法</li>
<li>计算出代价函数$J(\theta)$</li>
<li><p>执行反向传播算法<br><img src="/img/ML/p55.png" alt=""><br>具体来讲，我们对所有的m个训练样本使用for循环遍历，在这个for循环里，我们对这个样本进行前向和反向算法，这样就可以得到神经网络每一层中每一个单元对应的激励值和$\delta$项</p>
<p>接下来还是在for循环，我们要计算出$\Delta^{(l)}$</p>
<p>然后在循环外，我们将计算出J对参数的偏导数项，记得还要把正则化项lambda值考虑在内</p>
</li>
<li><p>梯度检查</p>
<p>  以保证我们的反向传播算法得到的结果是正确的</p>
</li>
<li><p>停用梯度检查</p>
</li>
<li><p>使用一个最优化算法，比如说梯度下降法或者更高级的优化算法，将这些优化算法和反向传播算法相结合。顺带一提，对于神经网络，代价函数是一个非凸函数，因此理论上可能停留在局部最小值，但影响不大，效果还是蛮好的</p>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
      </tags>
  </entry>
</search>
