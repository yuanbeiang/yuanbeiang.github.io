<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>markdown基本使用方法</title>
    <url>/2025/08/01/markdown%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h1 id="Markdown基本用法"><a href="#Markdown基本用法" class="headerlink" title="Markdown基本用法"></a>Markdown基本用法</h1><h2 id="1-标题"><a href="#1-标题" class="headerlink" title="1.标题"></a>1.标题</h2><p>井号的个数表示不同级的标题</p>
<h2 id="2-换行"><a href="#2-换行" class="headerlink" title="2.换行"></a>2.换行</h2><ol>
<li>换行注意要加两个空格，不然怕在其他渲染器里失效</li>
<li>如果中间空一行，那就是新起一段</li>
</ol>
<h2 id="3-强调（加粗和斜体）"><a href="#3-强调（加粗和斜体）" class="headerlink" title="3.强调（加粗和斜体）"></a>3.强调（加粗和斜体）</h2><ol>
<li>加粗：左右各加两个*</li>
<li>斜体：左右各加一个*</li>
<li>快捷键（前提安装了扩展）<ol>
<li>斜体：ctrl+i</li>
<li>加粗：ctrl+b</li>
</ol>
</li>
</ol>
<h2 id="4-列表"><a href="#4-列表" class="headerlink" title="4.列表"></a>4.列表</h2><ol>
<li>tab键可以继续缩进列表</li>
<li>列表编号你自己写的数字不用在意，系统会自动渲染成正确的顺序</li>
<li>如果要重新从1开始哦编号，只需要在两个列表之间加一个段落就行</li>
</ol>
<h2 id="5-图片"><a href="#5-图片" class="headerlink" title="5.图片"></a>5.图片</h2><p>先把图片保存到文件夹，然后输入![]（） 括号里在加上一个英文句号，然后就可以选择照片</p>
<h2 id="6-插入公式"><a href="#6-插入公式" class="headerlink" title="6.插入公式"></a>6.插入公式</h2><ol>
<li>用$$括起来即可</li>
<li>markdown all in one可以提供自动补全(怎么失效了┭┮﹏┭┮)</li>
<li>文字中插入公式：ctrl+m<br>注意在第一个美元符号前加空格，以保证兼容性</li>
<li>如果按两下ctrl+m，那就是单独一段的公式</li>
</ol>
<h2 id="7-表格"><a href="#7-表格" class="headerlink" title="7.表格"></a>7.表格</h2><ol>
<li>表格第一行代表标头</li>
<li>用  |   隔开</li>
<li>表头下面需要加上一行—-|—-|—-</li>
<li>表格默认左对齐，—-:右对齐，:—-:居中对齐</li>
<li>alt+shift+f 格式化，使表格在文本中变得好看一点</li>
</ol>
<h2 id="8-链接"><a href="#8-链接" class="headerlink" title="8.链接"></a>8.链接</h2><ol>
<li>把链接复制之后ctrl+v到选中的文字（安装扩展后）</li>
</ol>
<h2 id="9-代码块"><a href="#9-代码块" class="headerlink" title="9.代码块"></a>9.代码块</h2><ol>
<li>一对```</li>
<li>后面加上使用的编程语言名称即可实现高亮</li>
</ol>
<h2 id="10-导出为pdf"><a href="#10-导出为pdf" class="headerlink" title="10.导出为pdf"></a>10.导出为pdf</h2><p>按此方法能保证公式渲染正确</p>
<h2 id="11-其他语法"><a href="#11-其他语法" class="headerlink" title="11.其他语法"></a>11.其他语法</h2><p>这里是<a href="https://markdown.com.cn/basic-syntax/">链接</a><br>如果latex公式出现问题，请访问此<a href="https://rickliu.com/posts/f9538327001b/index.html">链接</a></p>
<blockquote>
<p>最后来张图片~<br><img src="/img/Yojiro.jpg" alt=""></p>
</blockquote>
]]></content>
      <categories>
        <category>傻瓜1号</category>
        <category>傻瓜2号</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>傻瓜</tag>
      </tags>
  </entry>
  <entry>
    <title>分类问题</title>
    <url>/2025/08/10/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><h3 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h3><p>要预测的变量y是一个离散值</p>
<ol>
<li><p>正类/负类</p>
<p>一般来说，负类表示没有某样东西</p>
</li>
<li><p>不推荐将线性回归用于分类问题</p>
</li>
</ol>
<hr>
<p>对于标签y为离散值0或1，我们使用分类算法——<strong>logistic回归算法</strong>  </p>
<p>特点是算法的输出或者说预测值一直介于0和1之间，不会大于1或者小于0</p>
<p>还有，虽然名字中带有“回归”二字，但是它是一种分类算法，叫做这个只是因为历史原因<br><img src="/img/ML/p4.png" alt=""></p>
<hr>
<h3 id="logistic回归"><a href="#logistic回归" class="headerlink" title="logistic回归"></a>logistic回归</h3><h4 id="part-1-假设函数的表示方法"><a href="#part-1-假设函数的表示方法" class="headerlink" title="part 1 假设函数的表示方法"></a>part 1 假设函数的表示方法</h4><p><img src="/img/ML/p5.png" alt=""><br><img src="/img/ML/p6.png" alt=""><br>假设函数解读：给定参数 $\theta$,对具有 $x$特征的病人， $y=1$的概率</p>
<h4 id="part-2-决策边界的概念"><a href="#part-2-决策边界的概念" class="headerlink" title="part 2 决策边界的概念"></a>part 2 决策边界的概念</h4><p><em>假设函数是如何做出预测的</em><br><img src="/img/ML/p7.png" alt=""></p>
<p><em>决策边界例子</em><br><img src="/img/ML/p8.png" alt=""><br><img src="/img/ML/p9.png" alt=""><br>注意，决策边界是假设函数的一个属性，由参数 $\theta$决定，<strong>它不是数据集的属性！</strong></p>
<h4 id="part-3-如何拟合logistic回归模型的参数-theta"><a href="#part-3-如何拟合logistic回归模型的参数-theta" class="headerlink" title="part 3 如何拟合logistic回归模型的参数 $\theta$"></a>part 3 如何拟合logistic回归模型的参数 $\theta$</h4><ol>
<li><p>代价函数<br><img src="/img/ML/p10.png" alt=""><br><em>保证是凸函数</em></p>
</li>
<li><p>找出使J($\theta$)最小的 $\theta$</p>
<p>我们要知道，假设的输出实际上就是在输入为$x$，并且以 $\theta$为参数时使 $y=1$的概率，</p>
<p>最小化代价函数的方法是——<strong>梯度下降法</strong><br><img src="/img/ML/p11.png" alt=""></p>
<p>如果有n个特征，那就有一个 $(n+1)$维参数向量 $\theta$。通过上面那个式子来同时更新所有 $\theta$的值（从0到n）</p>
<p>注意别把此处的更新规则与线性回归的混为一谈，二者看似形式一样，但是此处的假设函数 $h(\theta)$的定义已经发生了变化<br><img src="/img/ML/p12.png" alt=""></p>
<p>拓：不要用for循环实现参数更新，效率太低，用向量化来实现把n+1个参数同时更新</p>
</li>
</ol>
<h4 id="part-4-多类别分类问题"><a href="#part-4-多类别分类问题" class="headerlink" title="part 4 多类别分类问题"></a>part 4 多类别分类问题</h4><p><strong>通过“一对多”的分类算法实现</strong></p>
<p>比如说这个例子，我们要将训练集转化为三个独立的二元分类问题<br><img src="/img/ML/p13.png" alt=""></p>
<p>在三个分类器中运行输入x，然后选择 $h$最大的类别作为预测值</p>
<h4 id="part-5-过拟合问题"><a href="#part-5-过拟合问题" class="headerlink" title="part 5 过拟合问题"></a>part 5 过拟合问题</h4><p><img src="/img/ML/p14.png" alt=""><br><img src="/img/ML/p15.png" alt=""><br><img src="/img/ML/p16.png" alt=""></p>
<p>当我们使用一维或二维数据时，我们可以通过绘出假设模型的图像来研究问题所在，再选择合适的多项式阶数，<strong>但是这并不总是有用</strong></p>
<p>更多的时候，我们的学习问题需要有很多特征变量，并且这不仅仅是选择多项式阶次的问题，。当特征变量很多时，绘图变得更难，通过数据的可视化来决定保留哪些特征变量也更难</p>
<p>但是，如果我们有过多的变量而只有非常少的训练数据，就会出现<strong>过拟合</strong>的问题</p>
<p>有两个办法来解决过拟合的问题</p>
<p>第一个办法是尽量减少选取的变量，但不推荐！！！因为担心丢失了有用的信息</p>
<p>第二个才是我们主要的方法，<strong>正则化</strong></p>
<h4 id="part-6-正则化"><a href="#part-6-正则化" class="headerlink" title="part 6 正则化"></a>part 6 正则化</h4><p><strong>核心：我们将保留所有的特征变量，但是减少量级(或者说是参数 $\theta_j$的大小)</strong></p>
<p>当我们进行正则化的时候，我们还将写出相应的代价函数，关键是加入<strong>惩罚项</strong><br><img src="/img/ML/p17.png" alt=""><br>核心思想就是使参数尽量地小，以使假设模型更简单</p>
<p>由于在实际情况中，我们不知道到底哪些参数要缩小，所以我们要通过一个代价函数来对所有参数进行一个操作(在原来的代价函数后面添加一个新的项——<strong>正则化项</strong>，注意求和是从1开始，不从0开始)</p>
<p>$\lambda$被称为正则化参数，作用是控制两个不同目标之间的取舍——拟合数据与保持参数尽量地小(保持假设模型的相对简单，避免过拟合的情况)<br><img src="/img/ML/p18.png" alt=""><br><em>优化后的曲线并不是二次函数，但是却相对更平滑，更简单</em></p>
<p>如果$\lambda$被设得太大的话，那么对每个参数的惩罚程度就太大了，都趋近于0，最后只剩$\theta_0$，变成用一条直线去拟合数据了，这就是一个<strong>欠拟合</strong>的例子，因此，正则化参数$\lambda$的选择尤其重要，接下来我们就会讲到如何自动地选择它</p>
<h4 id="part-7-线性回归的正则化"><a href="#part-7-线性回归的正则化" class="headerlink" title="part 7 线性回归的正则化"></a>part 7 线性回归的正则化</h4><h5 id="算法一：梯度下降法"><a href="#算法一：梯度下降法" class="headerlink" title="算法一：梯度下降法"></a>算法一：梯度下降法</h5><p>把$\theta_0$的更新单独写出来，因为它不需要被惩罚</p>
<p>当我们进行正则化线性回归时，我们要做的就是每次迭代时，都将$\theta_j$乘以一个比1略小的数。从数学的角度来看，我们做的就是对代价函数进行梯度下降<br><img src="/img/ML/p19.png" alt=""></p>
<h5 id="算法二：正规方程"><a href="#算法二：正规方程" class="headerlink" title="算法二：正规方程"></a>算法二：正规方程</h5><p>我们的做法就是建立一个设计矩阵$X$，它的每一行都代表一个单独的训练样本。然后建立一个向量$y$，它是一个m维的向量，包含了训练集里的所有标签。</p>
<p>所以$X$是一个$m\times(n+1)$维的矩阵，y是一个m维的向量<br><img src="/img/ML/p20.png" alt=""></p>
<p>最后再来谈谈不可逆的问题，如果m小于n，那么$(X^TX)$是不可逆的(奇异矩阵，矩阵退化)</p>
<p>幸运的是，在正则化中已经考虑到了这个问题，<strong>只要正则化参数$\lambda$是严格大于0的，我们就可以确定$(X^TX)$+后面那个矩阵，得到的一定不是奇异矩阵，一定是可逆的！</strong><br><img src="/img/ML/p21.png" alt=""><br>因此，进行正则化还可以解决一些X的转置乘X出现不可逆的问题</p>
<p>好了，我们学会了实现正则化线性回归，利用它，我们就能避免出现过拟合的问题，即使在一个很小的训练集里拥有大量的特征</p>
<h4 id="part-8-逻辑回归的正则化"><a href="#part-8-逻辑回归的正则化" class="headerlink" title="part 8 逻辑回归的正则化"></a>part 8 逻辑回归的正则化</h4><p><img src="/img/ML/p22.png" alt=""><br>总体思路与线性回归的正则化中的梯度下降法相同，但是要注意虽然二者形式看似相同，但$h_\theta(x)$并不一样！</p>
<p>还有哦，刚才在线性回归里忘记强调了，方括号里的式子就是代价函数对$\theta_j$的偏导数</p>
]]></content>
      <categories>
        <category>傻瓜1号</category>
        <category>傻瓜2号</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>傻瓜</tag>
      </tags>
  </entry>
  <entry>
    <title>向量化</title>
    <url>/2025/08/10/%E5%90%91%E9%87%8F%E5%8C%96/</url>
    <content><![CDATA[<h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><ol>
<li>通过编程环境内置的或者说易于获取的线性代数库，而不是自己去编写，可以大大加快运行速度(尤其是在特征量非常大的时候)，并且更加有效利用计算机的并行硬件系统</li>
<li>代码更少，出错概率小!<br><img src="/img/ML/p2.png" alt=""><br><img src="/img/ML/p3.png" alt=""></li>
</ol>
]]></content>
      <categories>
        <category>傻瓜1号</category>
        <category>傻瓜2号</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>傻瓜</tag>
      </tags>
  </entry>
  <entry>
    <title>正规方程补充</title>
    <url>/2025/08/09/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E8%A1%A5%E5%85%85/</url>
    <content><![CDATA[<h2 id="正规方程在矩阵不可逆-奇异矩阵-的情况下的解决办法"><a href="#正规方程在矩阵不可逆-奇异矩阵-的情况下的解决办法" class="headerlink" title="正规方程在矩阵不可逆(奇异矩阵)的情况下的解决办法"></a>正规方程在矩阵不可逆(奇异矩阵)的情况下的解决办法</h2><p> 首先呢，这种情况极少数发生，而且即使发生了，如果用octave中的pinv，如果不可逆也能求出伪逆<br><img src="/img/ML/p1.png" alt=""></p>
<h3 id="为什么-X-TX-会出现不可逆的情况？"><a href="#为什么-X-TX-会出现不可逆的情况？" class="headerlink" title="为什么$X^TX$会出现不可逆的情况？"></a>为什么$X^TX$会出现不可逆的情况？</h3><ol>
<li>包含了多余的特征，比如一个以米为单位，另一个以英尺为单位，线性相关</li>
<li>运行的算法的特征太多了($m\le n$)<br>在此情况下，我们通常要删掉一些特征或者使用<strong>正则化</strong></li>
</ol>
]]></content>
      <categories>
        <category>傻瓜1号</category>
        <category>傻瓜2号</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>傻瓜</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络</title>
    <url>/2025/08/16/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="part-1-神经网络的基本概念"><a href="#part-1-神经网络的基本概念" class="headerlink" title="part 1 神经网络的基本概念"></a>part 1 神经网络的基本概念</h3><p>为什么已经有了线性回归和logistic回归算法，还需要研究神经网络？</p>
<p><strong>因为它在学习复杂的非线性假设上被证明是一种好得多的算法，即使输入特征空间或n很大，也能轻松搞定</strong></p>
<p><img src="/img/ML/p23.png" alt=""><br>$x_0$被称为偏置单元或偏置神经元</p>
<p>带有sigmoid或者logistic<strong>激活函数</strong>的人工神经元</p>
<p>激活函数是指非线性函数$g(z)$，$g(z)=\frac{1}{1+e^{-z}}$</p>
<p>$\theta$可以称为模型的参数，但在大多数文献中叫模型的<strong>权重</strong></p>
<p><img src="/img/ML/p24.png" alt=""><br>神经网络其实就是一组神经元连接在一起的集合</p>
<p>任何非输入层和非输出层，都是隐藏层</p>
<p><img src="/img/ML/p25.png" alt=""><br>$a^{(j)}_i$表示第j层第i个神经元或单元的<strong>激活项</strong>，激活项是指由一个具体神经元计算并输出的值</p>
<p>此外，我们的神经网络被这些矩阵参数化——$\theta^{(j)}$，即权重矩阵。它控制从某一层到下一层的映射</p>
<p>在此图，$\theta^{(1)}$就是控制着从三个输入单元到三个隐藏单元的映射的参数矩阵，它是一个3*4矩阵<br><img src="/img/ML/p26.png" alt=""></p>
<p>最后在输出层我们还有一个单元，它计算h(x),也可以写成$a^{(3)}_1$</p>
<p><strong>总结一下</strong>，我们讲解了这样一张图是如何定义一个人工神经网络的。其中的神经网络定义了函数h从输入x到输出y的映射。这些假设被参数化，记作$\Theta$，这样一来，改变$\Theta$就能得到不同的假设</p>
<h3 id="part-2-神经网络的具体实现"><a href="#part-2-神经网络的具体实现" class="headerlink" title="part 2 神经网络的具体实现"></a>part 2 神经网络的具体实现</h3><p>a和z的上标表示这些值与哪一层有关</p>
<p>这些z的值都是神经元输入值的加权线性组合(所谓加权，其实就是这些系数不是随便取，而是有特殊意义，常常被叫做“权重”。权重大，就影响大，权重小，就影响小<br>。。。。。哎呀就是x的系数是权重啦)</p>
<p><strong>前向传播：我们从输入单元的激活项开始，然后向前传播给隐藏层，计算隐藏层的激活项，然后我们继续前向传播，并计算输出层的激活项。这个依次计算激活项，从输入层到隐藏层再到输出层的过程叫前向传播</strong></p>
<p><img src="/img/ML/p27.png" alt=""></p>
<p>这张图推导出的是向前传播的向量化实现方法</p>
<p><a href="https://www.bilibili.com/video/BV164411b7dx?t=200.1&amp;p=46">图没看懂就点这个，精准空降</a></p>
<h3 id="part-3-前向传播如何帮助我们了解神经网络的作用和它为什么能帮助我们学习有趣的非线性假设函数"><a href="#part-3-前向传播如何帮助我们了解神经网络的作用和它为什么能帮助我们学习有趣的非线性假设函数" class="headerlink" title="part 3 前向传播如何帮助我们了解神经网络的作用和它为什么能帮助我们学习有趣的非线性假设函数"></a>part 3 前向传播如何帮助我们了解神经网络的作用和它为什么能帮助我们学习有趣的非线性假设函数</h3><h4 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h4><p><img src="/img/ML/p28.png" alt=""></p>
<p>把x遮住，你会发现它的形式与逻辑回归一样。区别在于，输入项由x变成a，这就带来了极大地可操作性，因为由x到a的参数可以自己设计。</p>
<p><img src="/img/ML/p29.png" alt=""><br>神经网络中神经元的连接方式被称为神经网络的<strong>架构</strong></p>
<h4 id="具体例子"><a href="#具体例子" class="headerlink" title="具体例子"></a>具体例子</h4><p><img src="/img/ML/p30.png" alt=""><br>我们想做的是学习一个非线性的判断边界来区分正样本和负样本</p>
<p>具体来说，我们需要计算目标函数，如图<br><img src="/img/ML/p31.png" alt=""></p>
<p>我们如何构建一个神经网络来拟合这样的训练集(XNOR)呢</p>
<h5 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h5><p>我们先从能够拟合AND运算的网络入手(只含单个神经元)<br><img src="/img/ML/p32.png" alt=""></p>
<p>首先我们要加一个偏置单元(+1单元)，然后对偏重/参数进行赋值</p>
<p>通过写出真值表，我们就能弄清楚逻辑函数的取值是怎样的</p>
<p>逻辑或同理<br><img src="/img/ML/p33.png" alt=""></p>
<h5 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h5><p>实现逻辑非运算，大体思想就是在预期得到非结果的变量前面放一个很大的<strong>负权重</strong><br><img src="/img/ML/p34.png" alt=""></p>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>为啥神经网络可以计算这种复杂的函数呢？</p>
<p>我们的输入都放在输入层，然后在中间放一个隐藏层，用来计算一些关于输入的略微复杂的功能，然后再继续增加一层，用于计算一个更复杂的非线性函数</p>
<p>当神经网络有很多层时，在第二层中有一些关于输入的相对简单的函数，第三层又在此基础上计算更加复杂的方程，再往后一层，计算的函数越来越复杂</p>
<h3 id="part-4-如何利用神经网络解决多类别分类问题"><a href="#part-4-如何利用神经网络解决多类别分类问题" class="headerlink" title="part 4 如何利用神经网络解决多类别分类问题"></a>part 4 如何利用神经网络解决多类别分类问题</h3><p>采用的方法本质上是<strong>一对多法的拓展</strong></p>
<p><img src="/img/ML/p34.png" alt=""><br>我们要做的是建立一个有四个输出单元的神经网络，现在神经网络的输出将是一个四维的向量</p>
<p>这其实就像一对多法，现在可以说我们有4个逻辑回归分类器，它们每一个都将识别图片中的物体是否是它那一种</p>
<p>$y^{（i）}$</p>
<p>每一个训练样本都由($x^{（i）}，y^{（i）}$)组成。其中$x^{（i）}$就是四种物体其中一种的图像，而$y^{（i）}$就是这些向量中的一个</p>
<p>我们希望找到一个办法，让神经网络的输出值$h_\Theta(x^{(i)})$约等于$y^{(i)}$，并且它俩在该例子中都是四维向量，分别代表四种不同的类别</p>
]]></content>
      <categories>
        <category>傻瓜1号</category>
        <category>傻瓜2号</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>傻瓜</tag>
      </tags>
  </entry>
</search>
