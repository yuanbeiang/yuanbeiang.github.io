<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>markdown基本使用方法</title>
    <url>/2025/08/01/markdown%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h1 id="Markdown基本用法"><a href="#Markdown基本用法" class="headerlink" title="Markdown基本用法"></a>Markdown基本用法</h1><h2 id="1-标题"><a href="#1-标题" class="headerlink" title="1.标题"></a>1.标题</h2><p>井号的个数表示不同级的标题</p>
<h2 id="2-换行"><a href="#2-换行" class="headerlink" title="2.换行"></a>2.换行</h2><ol>
<li>换行注意要加两个空格，不然怕在其他渲染器里失效</li>
<li>如果中间空一行，那就是新起一段</li>
</ol>
<h2 id="3-强调（加粗和斜体）"><a href="#3-强调（加粗和斜体）" class="headerlink" title="3.强调（加粗和斜体）"></a>3.强调（加粗和斜体）</h2><ol>
<li>加粗：左右各加两个*</li>
<li>斜体：左右各加一个*</li>
<li>快捷键（前提安装了扩展）<ol>
<li>斜体：ctrl+i</li>
<li>加粗：ctrl+b</li>
</ol>
</li>
</ol>
<h2 id="4-列表"><a href="#4-列表" class="headerlink" title="4.列表"></a>4.列表</h2><ol>
<li>tab键可以继续缩进列表</li>
<li>列表编号你自己写的数字不用在意，系统会自动渲染成正确的顺序</li>
<li>如果要重新从1开始哦编号，只需要在两个列表之间加一个段落就行</li>
</ol>
<h2 id="5-图片"><a href="#5-图片" class="headerlink" title="5.图片"></a>5.图片</h2><p>先把图片保存到文件夹，然后输入![]（） 括号里在加上一个英文句号，然后就可以选择照片</p>
<h2 id="6-插入公式"><a href="#6-插入公式" class="headerlink" title="6.插入公式"></a>6.插入公式</h2><ol>
<li>用$$括起来即可</li>
<li>markdown all in one可以提供自动补全(怎么失效了┭┮﹏┭┮)</li>
<li>文字中插入公式：ctrl+m<br>注意在第一个美元符号前加空格，以保证兼容性</li>
<li>如果按两下ctrl+m，那就是单独一段的公式</li>
</ol>
<h2 id="7-表格"><a href="#7-表格" class="headerlink" title="7.表格"></a>7.表格</h2><ol>
<li>表格第一行代表标头</li>
<li>用  |   隔开</li>
<li>表头下面需要加上一行—-|—-|—-</li>
<li>表格默认左对齐，—-:右对齐，:—-:居中对齐</li>
<li>alt+shift+f 格式化，使表格在文本中变得好看一点</li>
</ol>
<h2 id="8-链接"><a href="#8-链接" class="headerlink" title="8.链接"></a>8.链接</h2><ol>
<li>把链接复制之后ctrl+v到选中的文字（安装扩展后）</li>
</ol>
<h2 id="9-代码块"><a href="#9-代码块" class="headerlink" title="9.代码块"></a>9.代码块</h2><ol>
<li>一对```</li>
<li>后面加上使用的编程语言名称即可实现高亮</li>
</ol>
<h2 id="10-导出为pdf"><a href="#10-导出为pdf" class="headerlink" title="10.导出为pdf"></a>10.导出为pdf</h2><p>按此方法能保证公式渲染正确</p>
<h2 id="11-其他语法"><a href="#11-其他语法" class="headerlink" title="11.其他语法"></a>11.其他语法</h2><p>这里是<a href="https://markdown.com.cn/basic-syntax/">链接</a><br>如果latex公式出现问题，请访问此<a href="https://rickliu.com/posts/f9538327001b/index.html">链接</a></p>
<blockquote>
<p>最后来张图片~<br><img src="/img/Yojiro.jpg" alt=""></p>
</blockquote>
]]></content>
      <categories>
        <category>傻瓜1号</category>
        <category>傻瓜2号</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>傻瓜</tag>
      </tags>
  </entry>
  <entry>
    <title>分类问题</title>
    <url>/2025/08/10/%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><h3 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h3><p>要预测的变量y是一个离散值</p>
<ol>
<li><p>正类/负类</p>
<p>一般来说，负类表示没有某样东西</p>
</li>
<li><p>不推荐将线性回归用于分类问题</p>
</li>
</ol>
<hr>
<p>对于标签y为离散值0或1，我们使用分类算法——<strong>logistic回归算法</strong>  </p>
<p>特点是算法的输出或者说预测值一直介于0和1之间，不会大于1或者小于0</p>
<p>还有，虽然名字中带有“回归”二字，但是它是一种分类算法，叫做这个只是因为历史原因<br><img src="/img/ML/p4.png" alt=""></p>
<hr>
<h3 id="logistic回归"><a href="#logistic回归" class="headerlink" title="logistic回归"></a>logistic回归</h3><h4 id="part-1-假设函数的表示方法"><a href="#part-1-假设函数的表示方法" class="headerlink" title="part 1 假设函数的表示方法"></a>part 1 假设函数的表示方法</h4><p><img src="/img/ML/p5.png" alt=""><br><img src="/img/ML/p6.png" alt=""><br>假设函数解读：给定参数 $\theta$,对具有 $x$特征的病人， $y=1$的概率</p>
<h4 id="part-2-决策边界的概念"><a href="#part-2-决策边界的概念" class="headerlink" title="part 2 决策边界的概念"></a>part 2 决策边界的概念</h4><p><em>假设函数是如何做出预测的</em><br><img src="/img/ML/p7.png" alt=""></p>
<p><em>决策边界例子</em><br><img src="/img/ML/p8.png" alt=""><br><img src="/img/ML/p9.png" alt=""><br>注意，决策边界是假设函数的一个属性，由参数 $\theta$决定，<strong>它不是数据集的属性！</strong></p>
<h4 id="part-3-如何拟合logistic回归模型的参数-theta"><a href="#part-3-如何拟合logistic回归模型的参数-theta" class="headerlink" title="part 3 如何拟合logistic回归模型的参数 $\theta$"></a>part 3 如何拟合logistic回归模型的参数 $\theta$</h4><ol>
<li><p>代价函数<br><img src="/img/ML/p10.png" alt=""><br><em>保证是凸函数</em></p>
</li>
<li><p>找出使J($\theta$)最小的 $\theta$</p>
<p>我们要知道，假设的输出实际上就是在输入为$x$，并且以 $\theta$为参数时使 $y=1$的概率，</p>
<p>最小化代价函数的方法是——<strong>梯度下降法</strong><br><img src="/img/ML/p11.png" alt=""></p>
<p>如果有n个特征，那就有一个 $(n+1)$维参数向量 $\theta$。通过上面那个式子来同时更新所有 $\theta$的值（从0到n）</p>
<p>注意别把此处的更新规则与线性回归的混为一谈，二者看似形式一样，但是此处的假设函数 $h(\theta)$的定义已经发生了变化<br><img src="/img/ML/p12.png" alt=""></p>
<p>拓：不要用for循环实现参数更新，效率太低，用向量化来实现把n+1个参数同时更新</p>
</li>
</ol>
<h4 id="part-4-多类别分类问题"><a href="#part-4-多类别分类问题" class="headerlink" title="part 4 多类别分类问题"></a>part 4 多类别分类问题</h4><p><strong>通过“一对多”的分类算法实现</strong></p>
<p>比如说这个例子，我们要将训练集转化为三个独立的二元分类问题<br><img src="/img/ML/p13.png" alt=""></p>
<p>在三个分类器中运行输入x，然后选择 $h$最大的类别作为预测值</p>
<h4 id="part-5-过拟合问题"><a href="#part-5-过拟合问题" class="headerlink" title="part 5 过拟合问题"></a>part 5 过拟合问题</h4><p><img src="/img/ML/p14.png" alt=""><br><img src="/img/ML/p15.png" alt=""><br><img src="/img/ML/p16.png" alt=""></p>
<p>当我们使用一维或二维数据时，我们可以通过绘出假设模型的图像来研究问题所在，再选择合适的多项式阶数，<strong>但是这并不总是有用</strong></p>
<p>更多的时候，我们的学习问题需要有很多特征变量，并且这不仅仅是选择多项式阶次的问题，。当特征变量很多时，绘图变得更难，通过数据的可视化来决定保留哪些特征变量也更难</p>
<p>但是，如果我们有过多的变量而只有非常少的训练数据，就会出现<strong>过拟合</strong>的问题</p>
<p>有两个办法来解决过拟合的问题</p>
<p>第一个办法是尽量减少选取的变量，但不推荐！！！因为担心丢失了有用的信息</p>
<p>第二个才是我们主要的方法，<strong>正则化</strong></p>
<h4 id="part-6-正则化"><a href="#part-6-正则化" class="headerlink" title="part 6 正则化"></a>part 6 正则化</h4><p><strong>核心：我们将保留所有的特征变量，但是减少量级(或者说是参数 $\theta_j$的大小)</strong></p>
<p>当我们进行正则化的时候，我们还将写出相应的代价函数，关键是加入<strong>惩罚项</strong><br><img src="/img/ML/p17.png" alt=""><br>核心思想就是使参数尽量地小，以使假设模型更简单</p>
<p>由于在实际情况中，我们不知道到底哪些参数要缩小，所以我们要通过一个代价函数来对所有参数进行一个操作(在原来的代价函数后面添加一个新的项——<strong>正则化项</strong>，注意求和是从1开始，不从0开始)</p>
<p>$\lambda$被称为正则化参数，作用是控制两个不同目标之间的取舍——拟合数据与保持参数尽量地小(保持假设模型的相对简单，避免过拟合的情况)<br><img src="/img/ML/p18.png" alt=""><br><em>优化后的曲线并不是二次函数，但是却相对更平滑，更简单</em></p>
<p>如果$\lambda$被设得太大的话，那么对每个参数的惩罚程度就太大了，都趋近于0，最后只剩$\theta_0$，变成用一条直线去拟合数据了，这就是一个<strong>欠拟合</strong>的例子，因此，正则化参数$\lambda$的选择尤其重要，接下来我们就会讲到如何自动地选择它</p>
<h4 id="part-7-线性回归的正则化"><a href="#part-7-线性回归的正则化" class="headerlink" title="part 7 线性回归的正则化"></a>part 7 线性回归的正则化</h4><h5 id="算法一：梯度下降法"><a href="#算法一：梯度下降法" class="headerlink" title="算法一：梯度下降法"></a>算法一：梯度下降法</h5><p>把$\theta_0$的更新单独写出来，因为它不需要被惩罚</p>
<p>当我们进行正则化线性回归时，我们要做的就是每次迭代时，都将$\theta_j$乘以一个比1略小的数。从数学的角度来看，我们做的就是对代价函数进行梯度下降<br><img src="/img/ML/p19.png" alt=""></p>
<h5 id="算法二：正规方程"><a href="#算法二：正规方程" class="headerlink" title="算法二：正规方程"></a>算法二：正规方程</h5><p>我们的做法就是建立一个设计矩阵$X$，它的每一行都代表一个单独的训练样本。然后建立一个向量$y$，它是一个m维的向量，包含了训练集里的所有标签。</p>
<p>所以$X$是一个$m\times(n+1)$维的矩阵，y是一个m维的向量<br><img src="/img/ML/p20.png" alt=""></p>
<p>最后再来谈谈不可逆的问题，如果m小于n，那么$(X^TX)$是不可逆的(奇异矩阵，矩阵退化)</p>
<p>幸运的是，在正则化中已经考虑到了这个问题，<strong>只要正则化参数$\lambda$是严格大于0的，我们就可以确定$(X^TX)$+后面那个矩阵，得到的一定不是奇异矩阵，一定是可逆的！</strong><br><img src="/img/ML/p21.png" alt=""><br>因此，进行正则化还可以解决一些X的转置乘X出现不可逆的问题</p>
<p>好了，我们学会了实现正则化线性回归，利用它，我们就能避免出现过拟合的问题，即使在一个很小的训练集里拥有大量的特征</p>
<h4 id="part-8-逻辑回归的正则化"><a href="#part-8-逻辑回归的正则化" class="headerlink" title="part 8 逻辑回归的正则化"></a>part 8 逻辑回归的正则化</h4><p><img src="/img/ML/p22.png" alt=""><br>总体思路与线性回归的正则化中的梯度下降法相同，但是要注意虽然二者形式看似相同，但$h_\theta(x)$并不一样！</p>
<p>还有哦，刚才在线性回归里忘记强调了，方括号里的式子就是代价函数对$\theta_j$的偏导数</p>
]]></content>
      <categories>
        <category>傻瓜1号</category>
        <category>傻瓜2号</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>傻瓜</tag>
      </tags>
  </entry>
  <entry>
    <title>向量化</title>
    <url>/2025/08/10/%E5%90%91%E9%87%8F%E5%8C%96/</url>
    <content><![CDATA[<h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><ol>
<li>通过编程环境内置的或者说易于获取的线性代数库，而不是自己去编写，可以大大加快运行速度(尤其是在特征量非常大的时候)，并且更加有效利用计算机的并行硬件系统</li>
<li>代码更少，出错概率小!<br><img src="/img/ML/p2.png" alt=""><br><img src="/img/ML/p3.png" alt=""></li>
</ol>
]]></content>
      <categories>
        <category>傻瓜1号</category>
        <category>傻瓜2号</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>傻瓜</tag>
      </tags>
  </entry>
  <entry>
    <title>正规方程补充</title>
    <url>/2025/08/09/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E8%A1%A5%E5%85%85/</url>
    <content><![CDATA[<h2 id="正规方程在矩阵不可逆-奇异矩阵-的情况下的解决办法"><a href="#正规方程在矩阵不可逆-奇异矩阵-的情况下的解决办法" class="headerlink" title="正规方程在矩阵不可逆(奇异矩阵)的情况下的解决办法"></a>正规方程在矩阵不可逆(奇异矩阵)的情况下的解决办法</h2><p> 首先呢，这种情况极少数发生，而且即使发生了，如果用octave中的pinv，如果不可逆也能求出伪逆<br><img src="/img/ML/p1.png" alt=""></p>
<h3 id="为什么-X-TX-会出现不可逆的情况？"><a href="#为什么-X-TX-会出现不可逆的情况？" class="headerlink" title="为什么$X^TX$会出现不可逆的情况？"></a>为什么$X^TX$会出现不可逆的情况？</h3><ol>
<li>包含了多余的特征，比如一个以米为单位，另一个以英尺为单位，线性相关</li>
<li>运行的算法的特征太多了($m\le n$)<br>在此情况下，我们通常要删掉一些特征或者使用<strong>正则化</strong></li>
</ol>
]]></content>
      <categories>
        <category>傻瓜1号</category>
        <category>傻瓜2号</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>傻瓜</tag>
      </tags>
  </entry>
</search>
